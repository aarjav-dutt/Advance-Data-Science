{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREDIT CARD FRAUD DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from datetime import datetime \n",
    "from sklearn.metrics import roc_auc_score as auc \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"creditcards.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time spanning: 2.0 days\n",
      "0.173 % of all transactions are fraud. \n"
     ]
    }
   ],
   "source": [
    "print(\"Total time spanning: {:.1f} days\".format(df['Time'].max() / (3600 * 24.0)))\n",
    "print(\"{:.3f} % of all transactions are fraud. \".format(np.sum(df['Class']) / df.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      float64\n",
       "V1        float64\n",
       "V2        float64\n",
       "V3        float64\n",
       "V4        float64\n",
       "V5        float64\n",
       "V6        float64\n",
       "V7        float64\n",
       "V8        float64\n",
       "V9        float64\n",
       "V10       float64\n",
       "V11       float64\n",
       "V12       float64\n",
       "V13       float64\n",
       "V14       float64\n",
       "V15       float64\n",
       "V16       float64\n",
       "V17       float64\n",
       "V18       float64\n",
       "V19       float64\n",
       "V20       float64\n",
       "V21       float64\n",
       "V22       float64\n",
       "V23       float64\n",
       "V24       float64\n",
       "V25       float64\n",
       "V26       float64\n",
       "V27       float64\n",
       "V28       float64\n",
       "Amount    float64\n",
       "Class       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n",
      "C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "df['scaled_amount'] = std_scaler.fit_transform(df['Amount'].reshape(-1,1))\n",
    "df['scaled_time'] = std_scaler.fit_transform(df['Time'].reshape(-1,1))\n",
    "\n",
    "df.drop(['Time','Amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.244964</td>\n",
       "      <td>-1.996583</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.342475</td>\n",
       "      <td>-1.996583</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.160686</td>\n",
       "      <td>-1.996562</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.140534</td>\n",
       "      <td>-1.996562</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.073403</td>\n",
       "      <td>-1.996541</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "0       0.244964    -1.996583 -1.359807 -0.072781  2.536347  1.378155   \n",
       "1      -0.342475    -1.996583  1.191857  0.266151  0.166480  0.448154   \n",
       "2       1.160686    -1.996562 -1.358354 -1.340163  1.773209  0.379780   \n",
       "3       0.140534    -1.996562 -0.966272 -0.185226  1.792993 -0.863291   \n",
       "4      -0.073403    -1.996541 -1.158233  0.877737  1.548718  0.403034   \n",
       "\n",
       "         V5        V6        V7        V8  ...         V20       V21  \\\n",
       "0 -0.338321  0.462388  0.239599  0.098698  ...    0.251412 -0.018307   \n",
       "1  0.060018 -0.082361 -0.078803  0.085102  ...   -0.069083 -0.225775   \n",
       "2 -0.503198  1.800499  0.791461  0.247676  ...    0.524980  0.247998   \n",
       "3 -0.010309  1.247203  0.237609  0.377436  ...   -0.208038 -0.108300   \n",
       "4 -0.407193  0.095921  0.592941 -0.270533  ...    0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  Class  \n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_amount = df['scaled_amount']\n",
    "scaled_time = df['scaled_time']\n",
    "\n",
    "df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "df.insert(0, 'scaled_amount', scaled_amount)\n",
    "df.insert(1, 'scaled_time', scaled_time)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test split on time series, using first 75% as training/val, and last 25% as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_RATIO = 0.25\n",
    "df.sort_values('scaled_time', inplace = True)\n",
    "TRA_INDEX = int((1-TEST_RATIO) * df.shape[0])\n",
    "train_x = df.iloc[:TRA_INDEX, 1:-2].values\n",
    "train_y = df.iloc[:TRA_INDEX, -1].values\n",
    "\n",
    "test_x = df.iloc[TRA_INDEX:, 1:-2].values\n",
    "test_y = df.iloc[TRA_INDEX:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train examples: 213605, total fraud cases: 398, equal to 0.00186 of total cases. \n"
     ]
    }
   ],
   "source": [
    "print(\"Total train examples: {}, total fraud cases: {}, equal to {:.5f} of total cases. \".format(train_x.shape[0], np.sum(train_y), np.sum(train_y)/train_x.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test examples: 71202, total fraud cases: 94, equal to 0.00132 of total cases. \n"
     ]
    }
   ],
   "source": [
    "print(\"Total test examples: {}, total fraud cases: {}, equal to {:.5f} of total cases. \".format(test_x.shape[0], np.sum(test_y), np.sum(test_y)/test_y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_mean = []\n",
    "cols_std = []\n",
    "for c in range(train_x.shape[1]):\n",
    "    cols_mean.append(train_x[:,c].mean())\n",
    "    cols_std.append(train_x[:,c].std())\n",
    "    train_x[:, c] = (train_x[:, c] - cols_mean[-1]) / cols_std[-1]\n",
    "    test_x[:, c] =  (test_x[:, c] - cols_mean[-1]) / cols_std[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Modelling and results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auto-encoder as unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "batch_size = 256\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 15 # 1st layer num features\n",
    "#n_hidden_2 = 15 # 2nd layer num features\n",
    "n_input = train_x.shape[1] # MNIST data input (img shape: 28*28)\n",
    "data_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and val the model with 1 hidden layer - Tanh and RMSProp Optimizer- Gaussian Initialization-random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.171934247 Train auc= 0.951725 Time elapsed= 0:00:02.646789\n",
      "Epoch: 0002 cost= 0.717562079 Train auc= 0.957851 Time elapsed= 0:00:03.862451\n",
      "Epoch: 0003 cost= 0.494239509 Train auc= 0.960824 Time elapsed= 0:00:05.040737\n",
      "Epoch: 0004 cost= 0.836813748 Train auc= 0.960140 Time elapsed= 0:00:06.187692\n",
      "Epoch: 0005 cost= 0.751312673 Train auc= 0.961828 Time elapsed= 0:00:07.345155\n",
      "Epoch: 0006 cost= 0.401069373 Train auc= 0.962903 Time elapsed= 0:00:08.475809\n",
      "Epoch: 0007 cost= 0.555899382 Train auc= 0.962404 Time elapsed= 0:00:09.648231\n",
      "Epoch: 0008 cost= 0.446349353 Train auc= 0.961878 Time elapsed= 0:00:10.770531\n",
      "Epoch: 0009 cost= 0.459049284 Train auc= 0.961599 Time elapsed= 0:00:11.918812\n",
      "Epoch: 0010 cost= 0.351741850 Train auc= 0.961301 Time elapsed= 0:00:13.058618\n",
      "Optimization Finished!\n",
      "Model saved in file: .\\temp_saved_model_1layer.ckpt\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "weights = {\n",
    "   'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "  \n",
    "   'decoder_h1': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "  \n",
    "}\n",
    "biases = {\n",
    "   'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "  \n",
    "   'decoder_b1': tf.Variable(tf.random_normal([n_input])),\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "   # Encoder Hidden layer with tanh activation #1\n",
    "   layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                  biases['encoder_b1']))\n",
    " \n",
    "   return layer_1\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "   # Encoder Hidden layer with sigmoid activation #1\n",
    "   layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                  biases['decoder_b1']))\n",
    " \n",
    "   return layer_1\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define batch mse\n",
    "batch_mse = tf.reduce_mean(tf.pow(y_true - y_pred, 2), 1)\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# TRAIN StARTS\n",
    "save_model = os.path.join(data_dir, 'temp_saved_model_1layer.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "   now = datetime.now()\n",
    "   sess.run(init)\n",
    "   total_batch = int(train_x.shape[0]/batch_size)\n",
    "   # Training cycle\n",
    "   for epoch in range(training_epochs):\n",
    "       # Loop over all batches\n",
    "       for i in range(total_batch):\n",
    "           batch_idx = np.random.choice(train_x.shape[0], batch_size)\n",
    "           batch_xs = train_x[batch_idx]\n",
    "           # Run optimization op (backprop) and cost op (to get loss value)\n",
    "           _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs})\n",
    "           \n",
    "       # Display logs per epoch step\n",
    "       if epoch % display_step == 0:\n",
    "           train_batch_mse = sess.run(batch_mse, feed_dict={X: train_x})\n",
    "           print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                 \"cost=\", \"{:.9f}\".format(c),\n",
    "                 \"Train auc=\", \"{:.6f}\".format(auc(train_y, train_batch_mse)),\n",
    "                 \"Time elapsed=\", \"{}\".format(datetime.now() - now))\n",
    "\n",
    "   print(\"Optimization Finished!\")\n",
    "   \n",
    "   save_path = saver.save(sess, save_model)\n",
    "   print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get auto-encoder embedding (the `encoder_op` tensor) for both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .\\temp_saved_model_1layer.ckpt\n",
      "Dim for test_encoding and train_encoding are: \n",
      " (71202, 15) \n",
      " (213605, 15)\n"
     ]
    }
   ],
   "source": [
    "save_model = os.path.join(data_dir, 'temp_saved_model_1layer.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    now = datetime.now()\n",
    "    saver.restore(sess, save_model)\n",
    "    \n",
    "    test_encoding = sess.run(encoder_op, feed_dict={X: test_x})\n",
    "    train_encoding = sess.run(encoder_op, feed_dict={X: train_x})\n",
    "    \n",
    "    print(\"Dim for test_encoding and train_encoding are: \\n\", test_encoding.shape, '\\n', train_encoding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossEntropy-truncated_normal- ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#n_input = test_encoding.shape[1]\n",
    "n_input = test_encoding.shape[1]\n",
    "\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_input], name='input_x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None, output_size], name='target_y')\n",
    "\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.truncated_normal([n_input, hidden_size])),\n",
    "    'W2': tf.Variable(tf.truncated_normal([hidden_size, output_size])),\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros([hidden_size])),\n",
    "    'b2': tf.Variable(tf.zeros([output_size])),\n",
    "}\n",
    "\n",
    "hidden_layer =  tf.nn.relu(tf.add(tf.matmul(X, weights['W1']), biases['b1']))\n",
    "pred_logits = tf.add(tf.matmul(hidden_layer, weights['W2']), biases['b2'])\n",
    "pred_probs = tf.nn.softmax(pred_logits)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=pred_logits))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(2e-4).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data set. \n",
    "### Now we need to re-split the train into train/val, due to supervised training.\n",
    "### We will therefore use 80% of out previous training data as our new training, and the remaining 20% as new validation.  \n",
    "### AUC 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of data for train, val and test are: \n",
      "170884, \n",
      "42721, \n",
      "71202\n",
      "Epoch: 0001 cost= 1.544930935 Val auc= 0.824680 Time elapsed= 0:00:01.996616\n",
      "Epoch: 0002 cost= 0.813635230 Val auc= 0.839854 Time elapsed= 0:00:02.507533\n",
      "Epoch: 0003 cost= 0.488430321 Val auc= 0.801338 Time elapsed= 0:00:03.045914\n",
      "Epoch: 0004 cost= 0.306080401 Val auc= 0.784302 Time elapsed= 0:00:03.513796\n",
      "Epoch: 0005 cost= 0.222599864 Val auc= 0.765571 Time elapsed= 0:00:03.964445\n",
      "Epoch: 0006 cost= 0.148287237 Val auc= 0.465737 Time elapsed= 0:00:04.409756\n",
      "Epoch: 0007 cost= 0.103597090 Val auc= 0.452291 Time elapsed= 0:00:04.876539\n",
      "Epoch: 0008 cost= 0.116126716 Val auc= 0.634409 Time elapsed= 0:00:05.365797\n",
      "Epoch: 0009 cost= 0.041860372 Val auc= 0.843913 Time elapsed= 0:00:05.904763\n",
      "Epoch: 0010 cost= 0.027121577 Val auc= 0.869137 Time elapsed= 0:00:06.376883\n",
      "Epoch: 0011 cost= 0.016321372 Val auc= 0.873503 Time elapsed= 0:00:06.831097\n",
      "Epoch: 0012 cost= 0.013825809 Val auc= 0.877420 Time elapsed= 0:00:07.304806\n",
      "Epoch: 0013 cost= 0.044540655 Val auc= 0.879596 Time elapsed= 0:00:07.786853\n",
      "Epoch: 0014 cost= 0.005434199 Val auc= 0.880773 Time elapsed= 0:00:08.251471\n",
      "Epoch: 0015 cost= 0.005739101 Val auc= 0.882044 Time elapsed= 0:00:08.724230\n",
      "Epoch: 0016 cost= 0.003419946 Val auc= 0.883450 Time elapsed= 0:00:09.193102\n",
      "Epoch: 0017 cost= 0.002476718 Val auc= 0.883378 Time elapsed= 0:00:09.677569\n",
      "Epoch: 0018 cost= 0.003709343 Val auc= 0.886290 Time elapsed= 0:00:10.176044\n",
      "Epoch: 0019 cost= 0.039929040 Val auc= 0.888963 Time elapsed= 0:00:10.692801\n",
      "Epoch: 0020 cost= 0.003167790 Val auc= 0.892171 Time elapsed= 0:00:11.162764\n",
      "Epoch: 0021 cost= 0.001412985 Val auc= 0.892663 Time elapsed= 0:00:11.614448\n",
      "Epoch: 0022 cost= 0.027949387 Val auc= 0.898066 Time elapsed= 0:00:12.071945\n",
      "Epoch: 0023 cost= 0.000443323 Val auc= 0.899519 Time elapsed= 0:00:12.523189\n",
      "Epoch: 0024 cost= 0.001067063 Val auc= 0.901357 Time elapsed= 0:00:13.000082\n",
      "Epoch: 0025 cost= 0.001093042 Val auc= 0.904161 Time elapsed= 0:00:13.448279\n",
      "Epoch: 0026 cost= 0.000506094 Val auc= 0.907567 Time elapsed= 0:00:13.915327\n",
      "Epoch: 0027 cost= 0.000900526 Val auc= 0.911355 Time elapsed= 0:00:14.395807\n",
      "Epoch: 0028 cost= 0.000807311 Val auc= 0.913205 Time elapsed= 0:00:14.921720\n",
      "Epoch: 0029 cost= 0.000453431 Val auc= 0.917676 Time elapsed= 0:00:15.438998\n",
      "Epoch: 0030 cost= 0.000353410 Val auc= 0.920059 Time elapsed= 0:00:15.909894\n",
      "Epoch: 0031 cost= 0.001254721 Val auc= 0.921603 Time elapsed= 0:00:16.393173\n",
      "Epoch: 0032 cost= 0.002024231 Val auc= 0.923826 Time elapsed= 0:00:16.880203\n",
      "Epoch: 0033 cost= 0.000928189 Val auc= 0.924511 Time elapsed= 0:00:17.397486\n",
      "Epoch: 0034 cost= 0.001110232 Val auc= 0.925923 Time elapsed= 0:00:17.913069\n",
      "Epoch: 0035 cost= 0.001234944 Val auc= 0.927758 Time elapsed= 0:00:18.403181\n",
      "Epoch: 0036 cost= 0.007225485 Val auc= 0.929748 Time elapsed= 0:00:18.880886\n",
      "Epoch: 0037 cost= 0.000734280 Val auc= 0.931206 Time elapsed= 0:00:19.367514\n",
      "Epoch: 0038 cost= 0.000349091 Val auc= 0.932863 Time elapsed= 0:00:19.841211\n",
      "Epoch: 0039 cost= 0.000777287 Val auc= 0.934999 Time elapsed= 0:00:20.297845\n",
      "Epoch: 0040 cost= 0.000496004 Val auc= 0.935898 Time elapsed= 0:00:20.762511\n",
      "Epoch: 0041 cost= 0.001538980 Val auc= 0.937697 Time elapsed= 0:00:21.242240\n",
      "Epoch: 0042 cost= 0.000318705 Val auc= 0.938999 Time elapsed= 0:00:21.712919\n",
      "Epoch: 0043 cost= 0.001394428 Val auc= 0.939246 Time elapsed= 0:00:22.184490\n",
      "Epoch: 0044 cost= 0.001806242 Val auc= 0.940820 Time elapsed= 0:00:22.636362\n",
      "Epoch: 0045 cost= 0.001026854 Val auc= 0.941560 Time elapsed= 0:00:23.114736\n",
      "Epoch: 0046 cost= 0.001487955 Val auc= 0.941906 Time elapsed= 0:00:23.580973\n",
      "Epoch: 0047 cost= 0.000967634 Val auc= 0.942210 Time elapsed= 0:00:24.033884\n",
      "Epoch: 0048 cost= 0.000431939 Val auc= 0.943891 Time elapsed= 0:00:24.496677\n",
      "Epoch: 0049 cost= 0.001782176 Val auc= 0.944043 Time elapsed= 0:00:24.964694\n",
      "Epoch: 0050 cost= 0.000584728 Val auc= 0.945110 Time elapsed= 0:00:25.437170\n",
      "Epoch: 0051 cost= 0.000444307 Val auc= 0.944506 Time elapsed= 0:00:25.900131\n",
      "Epoch: 0052 cost= 0.000578016 Val auc= 0.945095 Time elapsed= 0:00:26.375455\n",
      "Epoch: 0053 cost= 0.004676179 Val auc= 0.945027 Time elapsed= 0:00:26.846255\n",
      "Epoch: 0054 cost= 0.001017703 Val auc= 0.945662 Time elapsed= 0:00:27.313207\n",
      "Epoch: 0055 cost= 0.001718035 Val auc= 0.945069 Time elapsed= 0:00:27.794686\n",
      "Epoch: 0056 cost= 0.000984154 Val auc= 0.945307 Time elapsed= 0:00:28.275710\n",
      "Epoch: 0057 cost= 0.000328862 Val auc= 0.946521 Time elapsed= 0:00:28.746248\n",
      "Epoch: 0058 cost= 0.000361673 Val auc= 0.947064 Time elapsed= 0:00:29.208708\n",
      "Epoch: 0059 cost= 0.001298619 Val auc= 0.947690 Time elapsed= 0:00:29.688339\n",
      "Epoch: 0060 cost= 0.000676196 Val auc= 0.947299 Time elapsed= 0:00:30.161165\n",
      "Epoch: 0061 cost= 0.000562769 Val auc= 0.947906 Time elapsed= 0:00:30.630673\n",
      "Epoch: 0062 cost= 0.002431230 Val auc= 0.948171 Time elapsed= 0:00:31.099578\n",
      "Epoch: 0063 cost= 0.000349912 Val auc= 0.947966 Time elapsed= 0:00:31.562774\n",
      "Epoch: 0064 cost= 0.030293336 Val auc= 0.947854 Time elapsed= 0:00:32.052186\n",
      "Epoch: 0065 cost= 0.000620595 Val auc= 0.947745 Time elapsed= 0:00:32.524484\n",
      "Epoch: 0066 cost= 0.000471580 Val auc= 0.948417 Time elapsed= 0:00:32.975331\n",
      "Epoch: 0067 cost= 0.007380653 Val auc= 0.948402 Time elapsed= 0:00:33.451451\n",
      "Epoch: 0068 cost= 0.000269140 Val auc= 0.948089 Time elapsed= 0:00:33.919533\n",
      "Epoch: 0069 cost= 0.000449618 Val auc= 0.949026 Time elapsed= 0:00:34.395727\n",
      "Epoch: 0070 cost= 0.000400289 Val auc= 0.949068 Time elapsed= 0:00:34.876861\n",
      "Epoch: 0071 cost= 0.000476325 Val auc= 0.948360 Time elapsed= 0:00:35.345681\n",
      "Epoch: 0072 cost= 0.000543854 Val auc= 0.948282 Time elapsed= 0:00:35.828441\n",
      "Epoch: 0073 cost= 0.000378904 Val auc= 0.948201 Time elapsed= 0:00:36.300882\n",
      "Epoch: 0074 cost= 0.000825254 Val auc= 0.948219 Time elapsed= 0:00:36.783941\n",
      "Epoch: 0075 cost= 0.001106706 Val auc= 0.948479 Time elapsed= 0:00:37.309491\n",
      "Epoch: 0076 cost= 0.000274328 Val auc= 0.948234 Time elapsed= 0:00:37.838862\n",
      "Epoch: 0077 cost= 0.000404370 Val auc= 0.949154 Time elapsed= 0:00:38.250096\n",
      "Epoch: 0078 cost= 0.001801502 Val auc= 0.948825 Time elapsed= 0:00:38.728417\n",
      "Epoch: 0079 cost= 0.000374723 Val auc= 0.948997 Time elapsed= 0:00:39.193978\n",
      "Epoch: 0080 cost= 0.000528820 Val auc= 0.949118 Time elapsed= 0:00:39.680086\n",
      "Optimization Finished!\n",
      "Model saved in file: .\\temp_saved_model_FCLayers.ckpt\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 80\n",
    "batch_size = 256\n",
    "\n",
    "# PREPARE DATA\n",
    "VAL_PERC = 0.2\n",
    "all_y_bin = np.zeros((df.shape[0], 2))\n",
    "all_y_bin[range(df.shape[0]), df['Class'].values] = 1\n",
    "\n",
    "train_enc_x = train_encoding[:int(train_encoding.shape[0] * (1-VAL_PERC))]\n",
    "train_enc_y = all_y_bin[:int(train_encoding.shape[0] * (1-VAL_PERC))]\n",
    "\n",
    "val_enc_x = train_encoding[int(train_encoding.shape[0] * (1-VAL_PERC)):]\n",
    "val_enc_y = all_y_bin[int(train_encoding.shape[0] * (1-VAL_PERC)):train_encoding.shape[0]]\n",
    "\n",
    "test_enc_y = all_y_bin[train_encoding.shape[0]:]\n",
    "print(\"Num of data for train, val and test are: \\n{}, \\n{}, \\n{}\".format(train_enc_x.shape[0], val_enc_x.shape[0], \\\n",
    "                                                                        test_encoding.shape[0]))\n",
    "\n",
    "# TRAIN STARTS\n",
    "save_model = os.path.join(data_dir, 'temp_saved_model_FCLayers.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    now = datetime.now()\n",
    "    sess.run(init)\n",
    "    total_batch = int(train_enc_x.shape[0]/batch_size)\n",
    "    # Training cycle\n",
    "    for epoch in range(n_epochs):\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_idx = np.random.choice(train_enc_x.shape[0], batch_size)\n",
    "            batch_xs = train_enc_x[batch_idx]\n",
    "            batch_ys = train_enc_y[batch_idx]\n",
    "\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cross_entropy], feed_dict={X: batch_xs, y_: batch_ys})\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            val_probs = sess.run(pred_probs, feed_dict={X: val_enc_x})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                  \"cost=\", \"{:.9f}\".format(c), \n",
    "                  \"Val auc=\", \"{:.6f}\".format(auc(val_enc_y[:, 1], val_probs[:, 1])), \n",
    "                  \"Time elapsed=\", \"{}\".format(datetime.now() - now))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    save_path = saver.save(sess, save_model)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model on the same test data as before - AUC remains constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .\\temp_saved_model_FCLayers.ckpt\n",
      "\n",
      "Test auc score: 0.9499190772442039\n"
     ]
    }
   ],
   "source": [
    "save_model = os.path.join(data_dir, 'temp_saved_model_FCLayers.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    now = datetime.now()\n",
    "    \n",
    "    saver.restore(sess, save_model)\n",
    "    \n",
    "    test_probs = sess.run(pred_probs, feed_dict={X: test_encoding})\n",
    "    \n",
    "    print(\"\\nTest auc score: {}\".format(auc(test_enc_y[:, 1], test_probs[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test a simple supervisied neural network with 2 layers - without using auto-encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax and Relu with Gradient estimation ADAM -changing the layer size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 256\n",
    "\n",
    "#n_input = test_encoding.shape[1]\n",
    "n_input = train_x.shape[1]\n",
    "\n",
    "hidden1_size = 8\n",
    "hidden2_size = 4\n",
    "output_size = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_input], name='input_x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None, output_size], name='target_y')\n",
    "\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.truncated_normal([n_input, hidden1_size])),\n",
    "    'W2': tf.Variable(tf.truncated_normal([hidden1_size, hidden2_size])),\n",
    "    'W3': tf.Variable(tf.truncated_normal([hidden2_size, output_size])),\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros([hidden1_size])),\n",
    "    'b2': tf.Variable(tf.zeros([hidden2_size])),\n",
    "    'b3': tf.Variable(tf.zeros([output_size])),\n",
    "}\n",
    "\n",
    "hidden1_layer =  tf.nn.relu(tf.add(tf.matmul(X, weights['W1']), biases['b1']))\n",
    "hidden2_layer =  tf.nn.relu(tf.add(tf.matmul(hidden1_layer, weights['W2']), biases['b2']))\n",
    "pred_logits = tf.add(tf.matmul(hidden2_layer, weights['W3']), biases['b3'])\n",
    "pred_probs = tf.nn.softmax(pred_logits)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=pred_logits))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data set. Now we need to re-split the train into train/val. \n",
    "### Again, we will use 80% of out previous training data as our new training, and the remaining 20% as new validation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of data for train, val and test are: \n",
      "170884, \n",
      "42721, \n",
      "71202\n",
      "Epoch: 0001 cost= 0.034505267 Val auc= 0.280303 Time elapsed= 0:00:02.578038\n",
      "Epoch: 0002 cost= 0.017247878 Val auc= 0.346173 Time elapsed= 0:00:03.158716\n",
      "Epoch: 0003 cost= 0.380723923 Val auc= 0.432901 Time elapsed= 0:00:03.738519\n",
      "Epoch: 0004 cost= 0.016374115 Val auc= 0.525157 Time elapsed= 0:00:04.306531\n",
      "Epoch: 0005 cost= 0.090899386 Val auc= 0.632393 Time elapsed= 0:00:04.898746\n",
      "Epoch: 0006 cost= 0.145710424 Val auc= 0.729861 Time elapsed= 0:00:05.501582\n",
      "Epoch: 0007 cost= 0.013912694 Val auc= 0.797550 Time elapsed= 0:00:06.087874\n",
      "Epoch: 0008 cost= 0.047216736 Val auc= 0.849654 Time elapsed= 0:00:06.683179\n",
      "Epoch: 0009 cost= 0.009366603 Val auc= 0.876225 Time elapsed= 0:00:07.257036\n",
      "Epoch: 0010 cost= 0.008545057 Val auc= 0.894060 Time elapsed= 0:00:07.828131\n",
      "Epoch: 0011 cost= 0.010808093 Val auc= 0.907660 Time elapsed= 0:00:08.418643\n",
      "Epoch: 0012 cost= 0.005629393 Val auc= 0.918120 Time elapsed= 0:00:08.996966\n",
      "Epoch: 0013 cost= 0.003585838 Val auc= 0.924407 Time elapsed= 0:00:09.566807\n",
      "Epoch: 0014 cost= 0.003469791 Val auc= 0.928860 Time elapsed= 0:00:10.142149\n",
      "Epoch: 0015 cost= 0.000980319 Val auc= 0.932050 Time elapsed= 0:00:10.721983\n",
      "Epoch: 0016 cost= 0.004129961 Val auc= 0.935376 Time elapsed= 0:00:11.305297\n",
      "Epoch: 0017 cost= 0.008861005 Val auc= 0.938527 Time elapsed= 0:00:11.973556\n",
      "Epoch: 0018 cost= 0.001093438 Val auc= 0.941886 Time elapsed= 0:00:12.550888\n",
      "Epoch: 0019 cost= 0.001721584 Val auc= 0.944448 Time elapsed= 0:00:13.117363\n",
      "Epoch: 0020 cost= 0.001737557 Val auc= 0.947271 Time elapsed= 0:00:13.728569\n",
      "Epoch: 0021 cost= 0.001402267 Val auc= 0.950073 Time elapsed= 0:00:14.357158\n",
      "Epoch: 0022 cost= 0.002921686 Val auc= 0.951542 Time elapsed= 0:00:15.056033\n",
      "Epoch: 0023 cost= 0.009322681 Val auc= 0.953046 Time elapsed= 0:00:15.643894\n",
      "Epoch: 0024 cost= 0.007972437 Val auc= 0.955006 Time elapsed= 0:00:16.274160\n",
      "Epoch: 0025 cost= 0.000772623 Val auc= 0.956332 Time elapsed= 0:00:16.892182\n",
      "Epoch: 0026 cost= 0.002968943 Val auc= 0.958223 Time elapsed= 0:00:17.516852\n",
      "Epoch: 0027 cost= 0.003159975 Val auc= 0.959646 Time elapsed= 0:00:18.141431\n",
      "Epoch: 0028 cost= 0.000844483 Val auc= 0.960619 Time elapsed= 0:00:18.744745\n",
      "Epoch: 0029 cost= 0.059238937 Val auc= 0.961285 Time elapsed= 0:00:19.412522\n",
      "Epoch: 0030 cost= 0.006991137 Val auc= 0.961654 Time elapsed= 0:00:20.023253\n",
      "Epoch: 0031 cost= 0.001351894 Val auc= 0.962110 Time elapsed= 0:00:20.662855\n",
      "Epoch: 0032 cost= 0.007382582 Val auc= 0.962757 Time elapsed= 0:00:21.285853\n",
      "Epoch: 0033 cost= 0.000926062 Val auc= 0.963326 Time elapsed= 0:00:21.873399\n",
      "Epoch: 0034 cost= 0.006838121 Val auc= 0.963554 Time elapsed= 0:00:22.486328\n",
      "Epoch: 0035 cost= 0.001140438 Val auc= 0.964331 Time elapsed= 0:00:23.160118\n",
      "Epoch: 0036 cost= 0.000320672 Val auc= 0.964509 Time elapsed= 0:00:23.756191\n",
      "Epoch: 0037 cost= 0.000477217 Val auc= 0.965175 Time elapsed= 0:00:24.420179\n",
      "Epoch: 0038 cost= 0.005723039 Val auc= 0.965593 Time elapsed= 0:00:25.033118\n",
      "Epoch: 0039 cost= 0.001141616 Val auc= 0.966196 Time elapsed= 0:00:25.642397\n",
      "Epoch: 0040 cost= 0.010253857 Val auc= 0.966133 Time elapsed= 0:00:26.270536\n",
      "Epoch: 0041 cost= 0.000517964 Val auc= 0.966192 Time elapsed= 0:00:26.839164\n",
      "Epoch: 0042 cost= 0.000498062 Val auc= 0.966869 Time elapsed= 0:00:27.451687\n",
      "Epoch: 0043 cost= 0.001251580 Val auc= 0.967196 Time elapsed= 0:00:28.108182\n",
      "Epoch: 0044 cost= 0.000754001 Val auc= 0.967311 Time elapsed= 0:00:28.728833\n",
      "Epoch: 0045 cost= 0.000587805 Val auc= 0.967297 Time elapsed= 0:00:29.356504\n",
      "Epoch: 0046 cost= 0.000726044 Val auc= 0.967857 Time elapsed= 0:00:29.988225\n",
      "Epoch: 0047 cost= 0.000769302 Val auc= 0.968263 Time elapsed= 0:00:30.570596\n",
      "Epoch: 0048 cost= 0.000271860 Val auc= 0.968216 Time elapsed= 0:00:31.146194\n",
      "Epoch: 0049 cost= 0.000500431 Val auc= 0.968683 Time elapsed= 0:00:31.745029\n",
      "Epoch: 0050 cost= 0.004958688 Val auc= 0.968872 Time elapsed= 0:00:32.306084\n",
      "Optimization Finished!\n",
      "Model saved in file: .\\temp_saved_model_FCNNets_raw.ckpt\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA\n",
    "VAL_PERC = 0.2\n",
    "all_y_bin = np.zeros((df.shape[0], 2))\n",
    "all_y_bin[range(df.shape[0]), df['Class'].values] = 1\n",
    "\n",
    "train_enc_x = train_x[:int(train_x.shape[0] * (1-VAL_PERC))]\n",
    "train_enc_y = all_y_bin[:int(train_x.shape[0] * (1-VAL_PERC))]\n",
    "\n",
    "val_enc_x = train_x[int(train_encoding.shape[0] *  (1-VAL_PERC)):]\n",
    "val_enc_y = all_y_bin[int(train_encoding.shape[0] * (1-VAL_PERC)):train_x.shape[0]]\n",
    "\n",
    "test_enc_y = all_y_bin[train_x.shape[0]:]\n",
    "\n",
    "print(\"Num of data for train, val and test are: \\n{}, \\n{}, \\n{}\".format(train_enc_x.shape[0], val_enc_x.shape[0], \\\n",
    "                                                                        test_encoding.shape[0]))\n",
    "# TRAIN STARTS\n",
    "save_model = os.path.join(data_dir, 'temp_saved_model_FCNNets_raw.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    now = datetime.now()\n",
    "    sess.run(init)\n",
    "    total_batch = int(train_enc_x.shape[0]/batch_size)\n",
    "    # Training cycle\n",
    "    for epoch in range(n_epochs):\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_idx = np.random.choice(train_enc_x.shape[0], batch_size)\n",
    "            batch_xs = train_enc_x[batch_idx]\n",
    "            batch_ys = train_enc_y[batch_idx]\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cross_entropy], feed_dict={X: batch_xs, y_: batch_ys})\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            val_probs = sess.run(pred_probs, feed_dict={X: val_enc_x})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                  \"cost=\", \"{:.9f}\".format(c), \n",
    "                  \"Val auc=\", \"{:.6f}\".format(auc(val_enc_y[:, 1], val_probs[:, 1])), \n",
    "                  \"Time elapsed=\", \"{}\".format(datetime.now() - now))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    save_path = saver.save(sess, save_model)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC with Cross Softmax Entropy= 97%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .\\temp_saved_model_FCNNets_raw.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Key Variable_57 not found in checkpoint\n\t [[Node: save_18/RestoreV2_159 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_18/Const_0, save_18/RestoreV2_159/tensor_names, save_18/RestoreV2_159/shape_and_slices)]]\n\nCaused by op 'save_18/RestoreV2_159', defined at:\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-104-2cd1385cfa2a>\", line 2, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1056, in __init__\n    self.build()\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 669, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Key Variable_57 not found in checkpoint\n\t [[Node: save_18/RestoreV2_159 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_18/Const_0, save_18/RestoreV2_159/tensor_names, save_18/RestoreV2_159/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Key Variable_57 not found in checkpoint\n\t [[Node: save_18/RestoreV2_159 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_18/Const_0, save_18/RestoreV2_159/tensor_names, save_18/RestoreV2_159/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-2cd1385cfa2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mnow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtest_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restoring parameters from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1457\u001b[1;33m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Key Variable_57 not found in checkpoint\n\t [[Node: save_18/RestoreV2_159 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_18/Const_0, save_18/RestoreV2_159/tensor_names, save_18/RestoreV2_159/shape_and_slices)]]\n\nCaused by op 'save_18/RestoreV2_159', defined at:\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-104-2cd1385cfa2a>\", line 2, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1056, in __init__\n    self.build()\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 669, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Key Variable_57 not found in checkpoint\n\t [[Node: save_18/RestoreV2_159 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_18/Const_0, save_18/RestoreV2_159/tensor_names, save_18/RestoreV2_159/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "save_model = os.path.join(data_dir, 'temp_saved_model_FCNNets_raw.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    now = datetime.now()\n",
    "    \n",
    "    saver.restore(sess, save_model)\n",
    "    \n",
    "    test_probs = sess.run(pred_probs, feed_dict={X: test_x})\n",
    "    \n",
    "    print(\"Test auc score: {}\".format(auc(test_enc_y[:, 1], test_probs[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge loss entropy with 40 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim for test_encoding and train_encoding are: \n",
      " (71202, 15) \n",
      " (213605, 15)\n"
     ]
    }
   ],
   "source": [
    " print(\"Dim for test_encoding and train_encoding are: \\n\", test_encoding.shape, '\\n', train_encoding.shape)\n",
    "#n_input = test_encoding.shape[1]\n",
    "n_input1 = test_encoding.shape[1]\n",
    "\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_input1], name='input_x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None, output_size], name='target_y')\n",
    "\n",
    "weights = {\n",
    "   'W1': tf.Variable(tf.truncated_normal([n_input1, hidden_size])),\n",
    "   'W2': tf.Variable(tf.truncated_normal([hidden_size, output_size])),\n",
    "}\n",
    "biases = {\n",
    "   'b1': tf.Variable(tf.zeros([hidden_size])),\n",
    "   'b2': tf.Variable(tf.zeros([output_size])),\n",
    "}\n",
    "\n",
    "hidden_layer =  tf.nn.relu(tf.add(tf.matmul(X, weights['W1']), biases['b1']))\n",
    "pred_logits = tf.add(tf.matmul(hidden_layer, weights['W2']), biases['b2'])\n",
    "pred_probs = tf.nn.softmax(pred_logits)\n",
    "\n",
    "cross_entropy1 = tf.losses.hinge_loss(labels=y_, logits=pred_logits)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(2e-4).minimize(cross_entropy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of data for train, val and test are: \n",
      "170884, \n",
      "42721, \n",
      "71202\n",
      "Epoch: 0001 cost= 0.539889216 Val auc= 0.236312 Time elapsed= 0:00:03.040883\n",
      "Epoch: 0002 cost= 0.226194620 Val auc= 0.271641 Time elapsed= 0:00:03.476229\n",
      "Epoch: 0003 cost= 0.095232077 Val auc= 0.289534 Time elapsed= 0:00:04.048875\n",
      "Epoch: 0004 cost= 0.062183004 Val auc= 0.284352 Time elapsed= 0:00:04.604354\n",
      "Epoch: 0005 cost= 0.033100408 Val auc= 0.288726 Time elapsed= 0:00:05.108110\n",
      "Epoch: 0006 cost= 0.038464151 Val auc= 0.313449 Time elapsed= 0:00:05.564357\n",
      "Epoch: 0007 cost= 0.021998513 Val auc= 0.380105 Time elapsed= 0:00:05.998992\n",
      "Epoch: 0008 cost= 0.021359678 Val auc= 0.496365 Time elapsed= 0:00:06.459550\n",
      "Epoch: 0009 cost= 0.014096240 Val auc= 0.709887 Time elapsed= 0:00:06.905600\n",
      "Epoch: 0010 cost= 0.002652615 Val auc= 0.830276 Time elapsed= 0:00:07.369985\n",
      "Epoch: 0011 cost= 0.001037055 Val auc= 0.865938 Time elapsed= 0:00:07.815244\n",
      "Epoch: 0012 cost= 0.009403804 Val auc= 0.882411 Time elapsed= 0:00:08.262003\n",
      "Epoch: 0013 cost= 0.000829311 Val auc= 0.886263 Time elapsed= 0:00:08.688561\n",
      "Epoch: 0014 cost= 0.001137179 Val auc= 0.886849 Time elapsed= 0:00:09.132098\n",
      "Epoch: 0015 cost= 0.006759171 Val auc= 0.889061 Time elapsed= 0:00:09.582758\n",
      "Epoch: 0016 cost= 0.000255634 Val auc= 0.891309 Time elapsed= 0:00:10.019933\n",
      "Epoch: 0017 cost= 0.019722931 Val auc= 0.893788 Time elapsed= 0:00:10.459284\n",
      "Epoch: 0018 cost= 0.007871319 Val auc= 0.897657 Time elapsed= 0:00:10.899965\n",
      "Epoch: 0019 cost= 0.000320106 Val auc= 0.900223 Time elapsed= 0:00:11.354984\n",
      "Epoch: 0020 cost= 0.000000000 Val auc= 0.904589 Time elapsed= 0:00:11.790781\n",
      "Epoch: 0021 cost= 0.000000000 Val auc= 0.905955 Time elapsed= 0:00:12.237129\n",
      "Epoch: 0022 cost= 0.000095517 Val auc= 0.905545 Time elapsed= 0:00:12.691233\n",
      "Epoch: 0023 cost= 0.000000000 Val auc= 0.906715 Time elapsed= 0:00:13.132578\n",
      "Epoch: 0024 cost= 0.000000000 Val auc= 0.908523 Time elapsed= 0:00:13.580141\n",
      "Epoch: 0025 cost= 0.010994419 Val auc= 0.908862 Time elapsed= 0:00:14.007208\n",
      "Epoch: 0026 cost= 0.000144194 Val auc= 0.911796 Time elapsed= 0:00:14.459624\n",
      "Epoch: 0027 cost= 0.005448882 Val auc= 0.909579 Time elapsed= 0:00:14.918696\n",
      "Epoch: 0028 cost= 0.007613400 Val auc= 0.907889 Time elapsed= 0:00:15.416227\n",
      "Epoch: 0029 cost= 0.009948902 Val auc= 0.909037 Time elapsed= 0:00:15.944900\n",
      "Epoch: 0030 cost= 0.014061436 Val auc= 0.909448 Time elapsed= 0:00:16.384698\n",
      "Epoch: 0031 cost= 0.003036334 Val auc= 0.908921 Time elapsed= 0:00:16.847263\n",
      "Epoch: 0032 cost= 0.000000000 Val auc= 0.907289 Time elapsed= 0:00:17.282821\n",
      "Epoch: 0033 cost= 0.000000000 Val auc= 0.907952 Time elapsed= 0:00:17.724939\n",
      "Epoch: 0034 cost= 0.022177557 Val auc= 0.904003 Time elapsed= 0:00:18.169700\n",
      "Epoch: 0035 cost= 0.007221527 Val auc= 0.903970 Time elapsed= 0:00:18.609402\n",
      "Epoch: 0036 cost= 0.004124546 Val auc= 0.904634 Time elapsed= 0:00:19.050381\n",
      "Epoch: 0037 cost= 0.003943161 Val auc= 0.901918 Time elapsed= 0:00:19.501196\n",
      "Epoch: 0038 cost= 0.003799715 Val auc= 0.898615 Time elapsed= 0:00:19.928259\n",
      "Epoch: 0039 cost= 0.010497983 Val auc= 0.897728 Time elapsed= 0:00:20.385259\n",
      "Epoch: 0040 cost= 0.000047655 Val auc= 0.897941 Time elapsed= 0:00:20.831501\n",
      "Epoch: 0041 cost= 0.006796559 Val auc= 0.897148 Time elapsed= 0:00:21.305100\n",
      "Epoch: 0042 cost= 0.003268814 Val auc= 0.894932 Time elapsed= 0:00:21.847542\n",
      "Epoch: 0043 cost= 0.000000000 Val auc= 0.892982 Time elapsed= 0:00:22.268149\n",
      "Epoch: 0044 cost= 0.000071848 Val auc= 0.887179 Time elapsed= 0:00:22.714460\n",
      "Epoch: 0045 cost= 0.000000000 Val auc= 0.892183 Time elapsed= 0:00:23.149275\n",
      "Epoch: 0046 cost= 0.007302565 Val auc= 0.891102 Time elapsed= 0:00:23.603126\n",
      "Epoch: 0047 cost= 0.002620490 Val auc= 0.893662 Time elapsed= 0:00:24.035630\n",
      "Epoch: 0048 cost= 0.007543807 Val auc= 0.892050 Time elapsed= 0:00:24.548010\n",
      "Epoch: 0049 cost= 0.002297640 Val auc= 0.891459 Time elapsed= 0:00:25.055875\n",
      "Epoch: 0050 cost= 0.000000000 Val auc= 0.890780 Time elapsed= 0:00:25.490851\n",
      "Epoch: 0051 cost= 0.000000000 Val auc= 0.892171 Time elapsed= 0:00:25.944773\n",
      "Epoch: 0052 cost= 0.000000000 Val auc= 0.891780 Time elapsed= 0:00:26.384038\n",
      "Epoch: 0053 cost= 0.007716004 Val auc= 0.890504 Time elapsed= 0:00:26.837608\n",
      "Epoch: 0054 cost= 0.006146085 Val auc= 0.891525 Time elapsed= 0:00:27.268696\n",
      "Epoch: 0055 cost= 0.006056650 Val auc= 0.887825 Time elapsed= 0:00:27.714252\n",
      "Epoch: 0056 cost= 0.000000000 Val auc= 0.889150 Time elapsed= 0:00:28.148644\n",
      "Epoch: 0057 cost= 0.000000000 Val auc= 0.887872 Time elapsed= 0:00:28.601074\n",
      "Epoch: 0058 cost= 0.000000000 Val auc= 0.886622 Time elapsed= 0:00:29.048932\n",
      "Epoch: 0059 cost= 0.000000000 Val auc= 0.888599 Time elapsed= 0:00:29.547025\n",
      "Epoch: 0060 cost= 0.007818056 Val auc= 0.887070 Time elapsed= 0:00:30.133581\n",
      "Epoch: 0061 cost= 0.000000000 Val auc= 0.883759 Time elapsed= 0:00:30.612042\n",
      "Epoch: 0062 cost= 0.001690240 Val auc= 0.883502 Time elapsed= 0:00:31.049148\n",
      "Epoch: 0063 cost= 0.000786862 Val auc= 0.880867 Time elapsed= 0:00:31.490129\n",
      "Epoch: 0064 cost= 0.000000000 Val auc= 0.877597 Time elapsed= 0:00:31.932663\n",
      "Epoch: 0065 cost= 0.000649905 Val auc= 0.882351 Time elapsed= 0:00:32.378889\n",
      "Epoch: 0066 cost= 0.000576132 Val auc= 0.881481 Time elapsed= 0:00:32.811722\n",
      "Epoch: 0067 cost= 0.000000000 Val auc= 0.882712 Time elapsed= 0:00:33.264997\n",
      "Epoch: 0068 cost= 0.000455060 Val auc= 0.884269 Time elapsed= 0:00:33.701105\n",
      "Epoch: 0069 cost= 0.000000000 Val auc= 0.881187 Time elapsed= 0:00:34.155407\n",
      "Epoch: 0070 cost= 0.000314924 Val auc= 0.881071 Time elapsed= 0:00:34.648073\n",
      "Optimization Finished!\n",
      "Model saved in file: .\\temp_saved_model_FCLayers.ckpt\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 70\n",
    "batch_size = 256\n",
    "\n",
    "# PREPARE DATA\n",
    "VAL_PERC = 0.2\n",
    "all_y_bin = np.zeros((df.shape[0], 2))\n",
    "all_y_bin[range(df.shape[0]), df['Class'].values] = 1\n",
    "\n",
    "train_enc_x = train_encoding[:int(train_encoding.shape[0] * (1-VAL_PERC))]\n",
    "train_enc_y = all_y_bin[:int(train_encoding.shape[0] * (1-VAL_PERC))]\n",
    "\n",
    "val_enc_x = train_encoding[int(train_encoding.shape[0] * (1-VAL_PERC)):]\n",
    "val_enc_y = all_y_bin[int(train_encoding.shape[0] * (1-VAL_PERC)):train_encoding.shape[0]]\n",
    "\n",
    "test_enc_y = all_y_bin[train_encoding.shape[0]:]\n",
    "print(\"Num of data for train, val and test are: \\n{}, \\n{}, \\n{}\".format(train_enc_x.shape[0], val_enc_x.shape[0], \\\n",
    "                                                                       test_encoding.shape[0]))\n",
    "\n",
    "# TRAIN STARTS\n",
    "save_model = os.path.join(data_dir, 'temp_saved_model_FCLayers.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "   now = datetime.now()\n",
    "   sess.run(init)\n",
    "   total_batch = int(train_enc_x.shape[0]/batch_size)\n",
    "   # Training cycle\n",
    "   for epoch in range(n_epochs):\n",
    "       # Loop over all batches\n",
    "       for i in range(total_batch):\n",
    "           batch_idx = np.random.choice(train_enc_x.shape[0], batch_size)\n",
    "           batch_xs = train_enc_x[batch_idx]\n",
    "           batch_ys = train_enc_y[batch_idx]\n",
    "\n",
    "           # Run optimization op (backprop) and cost op (to get loss value)\n",
    "           _, c = sess.run([optimizer, cross_entropy1], feed_dict={X: batch_xs, y_: batch_ys})\n",
    "           \n",
    "       # Display logs per epoch step\n",
    "       if epoch % display_step == 0:\n",
    "           val_probs = sess.run(pred_probs, feed_dict={X: val_enc_x})\n",
    "           print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                 \"cost=\", \"{:.9f}\".format(c),\n",
    "                 \"Val auc=\", \"{:.6f}\".format(auc(val_enc_y[:, 1], val_probs[:, 1])),\n",
    "                 \"Time elapsed=\", \"{}\".format(datetime.now() - now))\n",
    "\n",
    "   print(\"Optimization Finished!\")\n",
    "   \n",
    "   save_path = saver.save(sess, save_model)\n",
    "   print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## hinge loss entropy\n",
    "AUC:88\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test split on time series, using first 75% as training/val, and last 25% as test\n",
    "### Train and val the model with 1 hidden layer - Tanh and RMSProp Optimizer- Gaussian Initialization-random --Scaled(96) Unscaled(93)\n",
    "### AUC with Cross Softmax Entropy- Unscaled(92%), Scaled(97%)\n",
    "### AUC with Hinge loss entropy- Unscaled(87%), Scaled(88%)\n",
    "# Best accuracy achieved from-  Softmax and Relu with Gradient estimation ADAM Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "The code in the document by Venelin Valkov curiousily-https://github.com/curiousily/Credit-Card-Fraud-Detection-using-Autoencoders-in-Keras/blob/master/LICENSE is licensed under the MIT License https://opensource.org/licenses/MIT\n",
    "\n",
    "The code in the document by cloudacademy- https://github.com/cloudacademy/fraud-detection/blob/master/LICENSE\n",
    "is licensed under the Apache License  http://www.apache.org/licenses/\n",
    "\n",
    "The code in the document by Aymeric Damien- https://github.com/aymericdamien is licensed under the MIT License https://opensource.org/licenses/MIT\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
