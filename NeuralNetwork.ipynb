{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from datetime import datetime \n",
    "from sklearn.metrics import roc_auc_score as auc \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"creditcards.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time spanning: 2.0 days\n",
      "0.173 % of all transactions are fraud. \n"
     ]
    }
   ],
   "source": [
    "print(\"Total time spanning: {:.1f} days\".format(df['Time'].max() / (3600 * 24.0)))\n",
    "print(\"{:.3f} % of all transactions are fraud. \".format(np.sum(df['Class']) / df.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      float64\n",
       "V1        float64\n",
       "V2        float64\n",
       "V3        float64\n",
       "V4        float64\n",
       "V5        float64\n",
       "V6        float64\n",
       "V7        float64\n",
       "V8        float64\n",
       "V9        float64\n",
       "V10       float64\n",
       "V11       float64\n",
       "V12       float64\n",
       "V13       float64\n",
       "V14       float64\n",
       "V15       float64\n",
       "V16       float64\n",
       "V17       float64\n",
       "V18       float64\n",
       "V19       float64\n",
       "V20       float64\n",
       "V21       float64\n",
       "V22       float64\n",
       "V23       float64\n",
       "V24       float64\n",
       "V25       float64\n",
       "V26       float64\n",
       "V27       float64\n",
       "V28       float64\n",
       "Amount    float64\n",
       "Class       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test split on time series, using first 75% as training/val, and last 25% as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_RATIO = 0.25\n",
    "df.sort_values('Time', inplace = True)\n",
    "TRA_INDEX = int((1-TEST_RATIO) * df.shape[0])\n",
    "train_x = df.iloc[:TRA_INDEX, 1:-2].values\n",
    "train_y = df.iloc[:TRA_INDEX, -1].values\n",
    "\n",
    "test_x = df.iloc[TRA_INDEX:, 1:-2].values\n",
    "test_y = df.iloc[TRA_INDEX:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train examples: 213605, total fraud cases: 398, equal to 0.00186 of total cases. \n"
     ]
    }
   ],
   "source": [
    "print(\"Total train examples: {}, total fraud cases: {}, equal to {:.5f} of total cases. \".format(train_x.shape[0], np.sum(train_y), np.sum(train_y)/train_x.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test examples: 71202, total fraud cases: 94, equal to 0.00132 of total cases. \n"
     ]
    }
   ],
   "source": [
    "print(\"Total test examples: {}, total fraud cases: {}, equal to {:.5f} of total cases. \".format(test_x.shape[0], np.sum(test_y), np.sum(test_y)/test_y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_mean = []\n",
    "cols_std = []\n",
    "for c in range(train_x.shape[1]):\n",
    "    cols_mean.append(train_x[:,c].mean())\n",
    "    cols_std.append(train_x[:,c].std())\n",
    "    train_x[:, c] = (train_x[:, c] - cols_mean[-1]) / cols_std[-1]\n",
    "    test_x[:, c] =  (test_x[:, c] - cols_mean[-1]) / cols_std[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Modelling and results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Auto-encoder as unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "batch_size = 256\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 15 # 1st layer num features\n",
    "#n_hidden_2 = 15 # 2nd layer num features\n",
    "n_input = train_x.shape[1] # MNIST data input (img shape: 28*28)\n",
    "data_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and val the model with 1 hidden layer - Tanh and RMSProp Optimizer- Gaussian Initialization-random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.478872895 Train auc= 0.957891 Time elapsed= 0:00:02.320953\n",
      "Epoch: 0002 cost= 0.985197067 Train auc= 0.958765 Time elapsed= 0:00:05.270322\n",
      "Epoch: 0003 cost= 1.790869951 Train auc= 0.958700 Time elapsed= 0:00:07.611269\n",
      "Epoch: 0004 cost= 0.577246010 Train auc= 0.959454 Time elapsed= 0:00:09.791744\n",
      "Epoch: 0005 cost= 0.461763293 Train auc= 0.960296 Time elapsed= 0:00:11.992906\n",
      "Epoch: 0006 cost= 0.473216265 Train auc= 0.961003 Time elapsed= 0:00:14.230938\n",
      "Epoch: 0007 cost= 0.842373073 Train auc= 0.960281 Time elapsed= 0:00:16.427414\n",
      "Epoch: 0008 cost= 0.325677723 Train auc= 0.957533 Time elapsed= 0:00:18.637305\n",
      "Epoch: 0009 cost= 0.386690944 Train auc= 0.956865 Time elapsed= 0:00:20.898712\n",
      "Epoch: 0010 cost= 0.366356373 Train auc= 0.957167 Time elapsed= 0:00:23.277154\n",
      "Optimization Finished!\n",
      "Model saved in file: .\\temp_saved_model_1layer.ckpt\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])), \n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "    \n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_input])),\n",
    "}\n",
    "\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with tanh activation #1\n",
    "    layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1'])\n",
    "    return layer_1\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Encoder Hidden layer with tanh activation #1\n",
    "    layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    return layer_1\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define batch mse\n",
    "batch_mse = tf.reduce_mean(tf.pow(y_true - y_pred, 2), 1)\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# TRAIN StARTS\n",
    "save_model = os.path.join(data_dir, 'temp_saved_model_1layer.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    now = datetime.now()\n",
    "    sess.run(init)\n",
    "    total_batch = int(train_x.shape[0]/batch_size)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_idx = np.random.choice(train_x.shape[0], batch_size)\n",
    "            batch_xs = train_x[batch_idx]\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs})\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            train_batch_mse = sess.run(batch_mse, feed_dict={X: train_x})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                  \"cost=\", \"{:.9f}\".format(c), \n",
    "                  \"Train auc=\", \"{:.6f}\".format(auc(train_y, train_batch_mse)), \n",
    "                  \"Time elapsed=\", \"{}\".format(datetime.now() - now))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    save_path = saver.save(sess, save_model)\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Get auto-encoder embedding (the `encoder_op` tensor) for both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .\\temp_saved_model_1layer.ckpt\n",
      "Dim for test_encoding and train_encoding are: \n",
      " (71202, 15) \n",
      " (213605, 15)\n"
     ]
    }
   ],
   "source": [
    "save_model = os.path.join(data_dir, 'temp_saved_model_1layer.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    now = datetime.now()\n",
    "    saver.restore(sess, save_model)\n",
    "    \n",
    "    test_encoding = sess.run(encoder_op, feed_dict={X: test_x})\n",
    "    train_encoding = sess.run(encoder_op, feed_dict={X: train_x})\n",
    "    \n",
    "    print(\"Dim for test_encoding and train_encoding are: \\n\", test_encoding.shape, '\\n', train_encoding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) CrossEntropy-truncated_normal- ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#n_input = test_encoding.shape[1]\n",
    "n_input = test_encoding.shape[1]\n",
    "\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_input], name='input_x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None, output_size], name='target_y')\n",
    "\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.truncated_normal([n_input, hidden_size])),\n",
    "    'W2': tf.Variable(tf.truncated_normal([hidden_size, output_size])),\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros([hidden_size])),\n",
    "    'b2': tf.Variable(tf.zeros([output_size])),\n",
    "}\n",
    "\n",
    "hidden_layer =  tf.nn.relu(tf.add(tf.matmul(X, weights['W1']), biases['b1']))\n",
    "pred_logits = tf.add(tf.matmul(hidden_layer, weights['W2']), biases['b2'])\n",
    "pred_probs = tf.nn.softmax(pred_logits)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=pred_logits))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(2e-4).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Prepare the data set. \n",
    "### Now we need to re-split the train into train/val, due to supervised training.\n",
    "### We will therefore use 80% of out previous training data as our new training, and the remaining 20% as new validation.  \n",
    "### AUC 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of data for train, val and test are: \n",
      "170884, \n",
      "42721, \n",
      "71202\n",
      "Epoch: 0001 cost= 0.238808006 Val auc= 0.815804 Time elapsed= 0:00:01.172733\n",
      "Epoch: 0002 cost= 0.102786362 Val auc= 0.827126 Time elapsed= 0:00:02.013724\n",
      "Epoch: 0003 cost= 0.068595916 Val auc= 0.843399 Time elapsed= 0:00:02.876446\n",
      "Epoch: 0004 cost= 0.044349782 Val auc= 0.860015 Time elapsed= 0:00:03.749218\n",
      "Epoch: 0005 cost= 0.018551897 Val auc= 0.872920 Time elapsed= 0:00:04.619408\n",
      "Epoch: 0006 cost= 0.011627660 Val auc= 0.883144 Time elapsed= 0:00:05.473859\n",
      "Epoch: 0007 cost= 0.009519242 Val auc= 0.890153 Time elapsed= 0:00:06.348519\n",
      "Epoch: 0008 cost= 0.005414076 Val auc= 0.897618 Time elapsed= 0:00:07.200052\n",
      "Epoch: 0009 cost= 0.003358477 Val auc= 0.905037 Time elapsed= 0:00:08.067150\n",
      "Epoch: 0010 cost= 0.001853112 Val auc= 0.911442 Time elapsed= 0:00:08.934362\n",
      "Epoch: 0011 cost= 0.001572615 Val auc= 0.920207 Time elapsed= 0:00:09.815685\n",
      "Epoch: 0012 cost= 0.001371119 Val auc= 0.926485 Time elapsed= 0:00:10.668089\n",
      "Epoch: 0013 cost= 0.000781177 Val auc= 0.930708 Time elapsed= 0:00:11.500710\n",
      "Epoch: 0014 cost= 0.003061491 Val auc= 0.935274 Time elapsed= 0:00:12.275359\n",
      "Epoch: 0015 cost= 0.000699445 Val auc= 0.938977 Time elapsed= 0:00:13.100893\n",
      "Epoch: 0016 cost= 0.004876680 Val auc= 0.942598 Time elapsed= 0:00:13.822446\n",
      "Epoch: 0017 cost= 0.000746788 Val auc= 0.945743 Time elapsed= 0:00:14.523287\n",
      "Epoch: 0018 cost= 0.000992027 Val auc= 0.947573 Time elapsed= 0:00:15.271288\n",
      "Epoch: 0019 cost= 0.004225051 Val auc= 0.948889 Time elapsed= 0:00:16.093807\n",
      "Epoch: 0020 cost= 0.006428321 Val auc= 0.950722 Time elapsed= 0:00:16.942566\n",
      "Epoch: 0021 cost= 0.000494353 Val auc= 0.952086 Time elapsed= 0:00:17.838960\n",
      "Epoch: 0022 cost= 0.000374888 Val auc= 0.952710 Time elapsed= 0:00:18.671543\n",
      "Epoch: 0023 cost= 0.003525292 Val auc= 0.953058 Time elapsed= 0:00:19.550600\n",
      "Epoch: 0024 cost= 0.027202550 Val auc= 0.953593 Time elapsed= 0:00:20.390735\n",
      "Epoch: 0025 cost= 0.021720026 Val auc= 0.954608 Time elapsed= 0:00:21.370796\n",
      "Epoch: 0026 cost= 0.000755638 Val auc= 0.954858 Time elapsed= 0:00:22.233883\n",
      "Epoch: 0027 cost= 0.035177819 Val auc= 0.955399 Time elapsed= 0:00:23.114763\n",
      "Epoch: 0028 cost= 0.007481962 Val auc= 0.955742 Time elapsed= 0:00:24.015621\n",
      "Epoch: 0029 cost= 0.000682367 Val auc= 0.955885 Time elapsed= 0:00:24.937366\n",
      "Epoch: 0030 cost= 0.005295353 Val auc= 0.956506 Time elapsed= 0:00:25.801849\n",
      "Epoch: 0031 cost= 0.004693284 Val auc= 0.955975 Time elapsed= 0:00:26.663133\n",
      "Epoch: 0032 cost= 0.000641233 Val auc= 0.956224 Time elapsed= 0:00:27.500038\n",
      "Epoch: 0033 cost= 0.002609655 Val auc= 0.955570 Time elapsed= 0:00:28.383333\n",
      "Epoch: 0034 cost= 0.000499611 Val auc= 0.955721 Time elapsed= 0:00:29.240436\n",
      "Epoch: 0035 cost= 0.000380408 Val auc= 0.955099 Time elapsed= 0:00:30.015403\n",
      "Epoch: 0036 cost= 0.002292529 Val auc= 0.955500 Time elapsed= 0:00:30.842561\n",
      "Epoch: 0037 cost= 0.002082466 Val auc= 0.955667 Time elapsed= 0:00:31.667250\n",
      "Epoch: 0038 cost= 0.000430602 Val auc= 0.956132 Time elapsed= 0:00:32.583139\n",
      "Epoch: 0039 cost= 0.006666412 Val auc= 0.955836 Time elapsed= 0:00:33.489754\n",
      "Epoch: 0040 cost= 0.000377358 Val auc= 0.955807 Time elapsed= 0:00:34.412330\n",
      "Epoch: 0041 cost= 0.000466088 Val auc= 0.955793 Time elapsed= 0:00:35.322886\n",
      "Epoch: 0042 cost= 0.003730835 Val auc= 0.955608 Time elapsed= 0:00:36.242697\n",
      "Epoch: 0043 cost= 0.001841107 Val auc= 0.955417 Time elapsed= 0:00:37.146680\n",
      "Epoch: 0044 cost= 0.000538848 Val auc= 0.955451 Time elapsed= 0:00:38.070347\n",
      "Epoch: 0045 cost= 0.000272481 Val auc= 0.955280 Time elapsed= 0:00:38.923256\n",
      "Epoch: 0046 cost= 0.000309144 Val auc= 0.955090 Time elapsed= 0:00:39.723465\n",
      "Epoch: 0047 cost= 0.000341628 Val auc= 0.955171 Time elapsed= 0:00:40.425574\n",
      "Epoch: 0048 cost= 0.000469339 Val auc= 0.955450 Time elapsed= 0:00:40.917399\n",
      "Epoch: 0049 cost= 0.000213160 Val auc= 0.955597 Time elapsed= 0:00:41.513448\n",
      "Epoch: 0050 cost= 0.000358512 Val auc= 0.955617 Time elapsed= 0:00:42.385102\n",
      "Epoch: 0051 cost= 0.017072853 Val auc= 0.955293 Time elapsed= 0:00:43.096956\n",
      "Epoch: 0052 cost= 0.001899665 Val auc= 0.955097 Time elapsed= 0:00:43.764318\n",
      "Epoch: 0053 cost= 0.000323415 Val auc= 0.954490 Time elapsed= 0:00:44.581032\n",
      "Epoch: 0054 cost= 0.002151978 Val auc= 0.955559 Time elapsed= 0:00:45.374657\n",
      "Epoch: 0055 cost= 0.004825160 Val auc= 0.955168 Time elapsed= 0:00:46.153129\n",
      "Epoch: 0056 cost= 0.033737976 Val auc= 0.955091 Time elapsed= 0:00:47.039444\n",
      "Epoch: 0057 cost= 0.001461845 Val auc= 0.955256 Time elapsed= 0:00:47.924913\n",
      "Epoch: 0058 cost= 0.000340911 Val auc= 0.954473 Time elapsed= 0:00:48.742056\n",
      "Epoch: 0059 cost= 0.002818171 Val auc= 0.954587 Time elapsed= 0:00:49.577267\n",
      "Epoch: 0060 cost= 0.028175035 Val auc= 0.954675 Time elapsed= 0:00:50.414794\n",
      "Epoch: 0061 cost= 0.000574146 Val auc= 0.954709 Time elapsed= 0:00:51.313601\n",
      "Epoch: 0062 cost= 0.000425456 Val auc= 0.955329 Time elapsed= 0:00:52.164521\n",
      "Epoch: 0063 cost= 0.030738363 Val auc= 0.954350 Time elapsed= 0:00:53.018594\n",
      "Epoch: 0064 cost= 0.007235027 Val auc= 0.954774 Time elapsed= 0:00:53.815411\n",
      "Epoch: 0065 cost= 0.006025265 Val auc= 0.954631 Time elapsed= 0:00:54.618755\n",
      "Epoch: 0066 cost= 0.000696368 Val auc= 0.954161 Time elapsed= 0:00:55.411997\n",
      "Epoch: 0067 cost= 0.000273153 Val auc= 0.953747 Time elapsed= 0:00:56.300843\n",
      "Epoch: 0068 cost= 0.001508293 Val auc= 0.953991 Time elapsed= 0:00:57.195714\n",
      "Epoch: 0069 cost= 0.000404557 Val auc= 0.954179 Time elapsed= 0:00:58.006379\n",
      "Epoch: 0070 cost= 0.045936368 Val auc= 0.954319 Time elapsed= 0:00:58.837763\n",
      "Epoch: 0071 cost= 0.001418988 Val auc= 0.954328 Time elapsed= 0:00:59.757830\n",
      "Epoch: 0072 cost= 0.000439902 Val auc= 0.953959 Time elapsed= 0:01:00.565466\n",
      "Epoch: 0073 cost= 0.000290462 Val auc= 0.954064 Time elapsed= 0:01:01.465049\n",
      "Epoch: 0074 cost= 0.016250603 Val auc= 0.954740 Time elapsed= 0:01:02.386293\n",
      "Epoch: 0075 cost= 0.000351337 Val auc= 0.954311 Time elapsed= 0:01:03.308062\n",
      "Epoch: 0076 cost= 0.005964441 Val auc= 0.953912 Time elapsed= 0:01:04.240420\n",
      "Epoch: 0077 cost= 0.001280354 Val auc= 0.954065 Time elapsed= 0:01:05.052089\n",
      "Epoch: 0078 cost= 0.000225467 Val auc= 0.953840 Time elapsed= 0:01:05.861188\n",
      "Epoch: 0079 cost= 0.000687525 Val auc= 0.953374 Time elapsed= 0:01:06.561909\n",
      "Epoch: 0080 cost= 0.046506267 Val auc= 0.954235 Time elapsed= 0:01:07.282049\n",
      "Optimization Finished!\n",
      "Model saved in file: .\\temp_saved_model_FCLayers.ckpt\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 80\n",
    "batch_size = 256\n",
    "\n",
    "# PREPARE DATA\n",
    "VAL_PERC = 0.2\n",
    "all_y_bin = np.zeros((df.shape[0], 2))\n",
    "all_y_bin[range(df.shape[0]), df['Class'].values] = 1\n",
    "\n",
    "train_enc_x = train_encoding[:int(train_encoding.shape[0] * (1-VAL_PERC))]\n",
    "train_enc_y = all_y_bin[:int(train_encoding.shape[0] * (1-VAL_PERC))]\n",
    "\n",
    "val_enc_x = train_encoding[int(train_encoding.shape[0] * (1-VAL_PERC)):]\n",
    "val_enc_y = all_y_bin[int(train_encoding.shape[0] * (1-VAL_PERC)):train_encoding.shape[0]]\n",
    "\n",
    "test_enc_y = all_y_bin[train_encoding.shape[0]:]\n",
    "print(\"Num of data for train, val and test are: \\n{}, \\n{}, \\n{}\".format(train_enc_x.shape[0], val_enc_x.shape[0], \\\n",
    "                                                                        test_encoding.shape[0]))\n",
    "\n",
    "# TRAIN STARTS\n",
    "save_model = os.path.join(data_dir, 'temp_saved_model_FCLayers.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    now = datetime.now()\n",
    "    sess.run(init)\n",
    "    total_batch = int(train_enc_x.shape[0]/batch_size)\n",
    "    # Training cycle\n",
    "    for epoch in range(n_epochs):\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_idx = np.random.choice(train_enc_x.shape[0], batch_size)\n",
    "            batch_xs = train_enc_x[batch_idx]\n",
    "            batch_ys = train_enc_y[batch_idx]\n",
    "\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cross_entropy], feed_dict={X: batch_xs, y_: batch_ys})\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            val_probs = sess.run(pred_probs, feed_dict={X: val_enc_x})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                  \"cost=\", \"{:.9f}\".format(c), \n",
    "                  \"Val auc=\", \"{:.6f}\".format(auc(val_enc_y[:, 1], val_probs[:, 1])), \n",
    "                  \"Time elapsed=\", \"{}\".format(datetime.now() - now))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    save_path = saver.save(sess, save_model)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Test the model on the same test data as before - AUC decreased - 0.93\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .\\temp_saved_model_FCLayers.ckpt\n",
      "\n",
      "Test auc score: 0.9396701331747094\n"
     ]
    }
   ],
   "source": [
    "save_model = os.path.join(data_dir, 'temp_saved_model_FCLayers.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    now = datetime.now()\n",
    "    \n",
    "    saver.restore(sess, save_model)\n",
    "    \n",
    "    test_probs = sess.run(pred_probs, feed_dict={X: test_encoding})\n",
    "    \n",
    "    print(\"\\nTest auc score: {}\".format(auc(test_enc_y[:, 1], test_probs[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test a simple supervisied neural network with 2 layers - without using auto-encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax and Relu with Gradient estimation ADAM -changing the layer size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 256\n",
    "\n",
    "#n_input = test_encoding.shape[1]\n",
    "n_input = train_x.shape[1]\n",
    "\n",
    "hidden1_size = 8\n",
    "hidden2_size = 4\n",
    "output_size = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_input], name='input_x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None, output_size], name='target_y')\n",
    "\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.truncated_normal([n_input, hidden1_size])),\n",
    "    'W2': tf.Variable(tf.truncated_normal([hidden1_size, hidden2_size])),\n",
    "    'W3': tf.Variable(tf.truncated_normal([hidden2_size, output_size])),\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros([hidden1_size])),\n",
    "    'b2': tf.Variable(tf.zeros([hidden2_size])),\n",
    "    'b3': tf.Variable(tf.zeros([output_size])),\n",
    "}\n",
    "\n",
    "hidden1_layer =  tf.nn.relu(tf.add(tf.matmul(X, weights['W1']), biases['b1']))\n",
    "hidden2_layer =  tf.nn.relu(tf.add(tf.matmul(hidden1_layer, weights['W2']), biases['b2']))\n",
    "pred_logits = tf.add(tf.matmul(hidden2_layer, weights['W3']), biases['b3'])\n",
    "pred_probs = tf.nn.softmax(pred_logits)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=pred_logits))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data set. Now we need to re-split the train into train/val. \n",
    "### Again, we will use 80% of out previous training data as our new training, and the remaining 20% as new validation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of data for train, val and test are: \n",
      "170884, \n",
      "42721, \n",
      "71202\n",
      "Epoch: 0001 cost= 0.273930579 Val auc= 0.134462 Time elapsed= 0:00:01.694410\n",
      "Epoch: 0002 cost= 0.053971462 Val auc= 0.122343 Time elapsed= 0:00:02.674651\n",
      "Epoch: 0003 cost= 0.573769450 Val auc= 0.122879 Time elapsed= 0:00:03.703272\n",
      "Epoch: 0004 cost= 0.376451492 Val auc= 0.135745 Time elapsed= 0:00:04.860840\n",
      "Epoch: 0005 cost= 0.024410944 Val auc= 0.159853 Time elapsed= 0:00:05.976339\n",
      "Epoch: 0006 cost= 0.175251752 Val auc= 0.193141 Time elapsed= 0:00:07.086186\n",
      "Epoch: 0007 cost= 0.015292783 Val auc= 0.241998 Time elapsed= 0:00:08.231790\n",
      "Epoch: 0008 cost= 0.014697665 Val auc= 0.286177 Time elapsed= 0:00:09.338120\n",
      "Epoch: 0009 cost= 0.010906655 Val auc= 0.331807 Time elapsed= 0:00:10.459901\n",
      "Epoch: 0010 cost= 0.034842875 Val auc= 0.384543 Time elapsed= 0:00:11.596600\n",
      "Epoch: 0011 cost= 0.007506951 Val auc= 0.431591 Time elapsed= 0:00:12.667259\n",
      "Epoch: 0012 cost= 0.059454374 Val auc= 0.481831 Time elapsed= 0:00:13.752805\n",
      "Epoch: 0013 cost= 0.007158935 Val auc= 0.523341 Time elapsed= 0:00:14.739996\n",
      "Epoch: 0014 cost= 0.092762545 Val auc= 0.565230 Time elapsed= 0:00:15.743586\n",
      "Epoch: 0015 cost= 0.004802634 Val auc= 0.605935 Time elapsed= 0:00:16.749578\n",
      "Epoch: 0016 cost= 0.004847034 Val auc= 0.646643 Time elapsed= 0:00:17.775093\n",
      "Epoch: 0017 cost= 0.030346109 Val auc= 0.688621 Time elapsed= 0:00:18.978121\n",
      "Epoch: 0018 cost= 0.003749216 Val auc= 0.724461 Time elapsed= 0:00:20.190023\n",
      "Epoch: 0019 cost= 0.015072117 Val auc= 0.753257 Time elapsed= 0:00:21.340607\n",
      "Epoch: 0020 cost= 0.030761136 Val auc= 0.774564 Time elapsed= 0:00:22.315718\n",
      "Epoch: 0021 cost= 0.002615135 Val auc= 0.790424 Time elapsed= 0:00:23.486347\n",
      "Epoch: 0022 cost= 0.176864833 Val auc= 0.800512 Time elapsed= 0:00:24.605941\n",
      "Epoch: 0023 cost= 0.020527514 Val auc= 0.813009 Time elapsed= 0:00:25.683026\n",
      "Epoch: 0024 cost= 0.010062942 Val auc= 0.820193 Time elapsed= 0:00:26.687453\n",
      "Epoch: 0025 cost= 0.035321269 Val auc= 0.830675 Time elapsed= 0:00:27.753857\n",
      "Epoch: 0026 cost= 0.034624249 Val auc= 0.836933 Time elapsed= 0:00:28.719378\n",
      "Epoch: 0027 cost= 0.001655723 Val auc= 0.844060 Time elapsed= 0:00:29.753106\n",
      "Epoch: 0028 cost= 0.001870484 Val auc= 0.852149 Time elapsed= 0:00:30.706458\n",
      "Epoch: 0029 cost= 0.022622535 Val auc= 0.857634 Time elapsed= 0:00:31.726624\n",
      "Epoch: 0030 cost= 0.001979687 Val auc= 0.862763 Time elapsed= 0:00:32.833582\n",
      "Epoch: 0031 cost= 0.001598463 Val auc= 0.868465 Time elapsed= 0:00:33.926832\n",
      "Epoch: 0032 cost= 0.001368060 Val auc= 0.874486 Time elapsed= 0:00:35.036260\n",
      "Epoch: 0033 cost= 0.001564705 Val auc= 0.879649 Time elapsed= 0:00:36.175552\n",
      "Epoch: 0034 cost= 0.019097697 Val auc= 0.884170 Time elapsed= 0:00:37.235722\n",
      "Epoch: 0035 cost= 0.037001614 Val auc= 0.888129 Time elapsed= 0:00:38.314241\n",
      "Epoch: 0036 cost= 0.001127398 Val auc= 0.887161 Time elapsed= 0:00:39.358630\n",
      "Epoch: 0037 cost= 0.001859397 Val auc= 0.891282 Time elapsed= 0:00:40.383942\n",
      "Epoch: 0038 cost= 0.000990531 Val auc= 0.893359 Time elapsed= 0:00:41.550379\n",
      "Epoch: 0039 cost= 0.001558839 Val auc= 0.899243 Time elapsed= 0:00:42.722278\n",
      "Epoch: 0040 cost= 0.008946396 Val auc= 0.897763 Time elapsed= 0:00:43.724806\n",
      "Epoch: 0041 cost= 0.001635971 Val auc= 0.903234 Time elapsed= 0:00:44.874269\n",
      "Epoch: 0042 cost= 0.020805905 Val auc= 0.901870 Time elapsed= 0:00:46.004994\n",
      "Epoch: 0043 cost= 0.032570858 Val auc= 0.906899 Time elapsed= 0:00:47.146767\n",
      "Epoch: 0044 cost= 0.001565742 Val auc= 0.910128 Time elapsed= 0:00:48.298889\n",
      "Epoch: 0045 cost= 0.039896786 Val auc= 0.916133 Time elapsed= 0:00:49.456365\n",
      "Epoch: 0046 cost= 0.015098460 Val auc= 0.918323 Time elapsed= 0:00:50.630796\n",
      "Epoch: 0047 cost= 0.007846480 Val auc= 0.920336 Time elapsed= 0:00:51.712266\n",
      "Epoch: 0048 cost= 0.000842684 Val auc= 0.922762 Time elapsed= 0:00:52.638833\n",
      "Epoch: 0049 cost= 0.000909692 Val auc= 0.924397 Time elapsed= 0:00:53.613710\n",
      "Epoch: 0050 cost= 0.001206317 Val auc= 0.926389 Time elapsed= 0:00:54.561121\n",
      "Optimization Finished!\n",
      "Model saved in file: .\\temp_saved_model_FCNNets_raw.ckpt\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA\n",
    "VAL_PERC = 0.2\n",
    "all_y_bin = np.zeros((df.shape[0], 2))\n",
    "all_y_bin[range(df.shape[0]), df['Class'].values] = 1\n",
    "\n",
    "train_enc_x = train_x[:int(train_x.shape[0] * (1-VAL_PERC))]\n",
    "train_enc_y = all_y_bin[:int(train_x.shape[0] * (1-VAL_PERC))]\n",
    "\n",
    "val_enc_x = train_x[int(train_encoding.shape[0] *  (1-VAL_PERC)):]\n",
    "val_enc_y = all_y_bin[int(train_encoding.shape[0] * (1-VAL_PERC)):train_x.shape[0]]\n",
    "\n",
    "test_enc_y = all_y_bin[train_x.shape[0]:]\n",
    "\n",
    "print(\"Num of data for train, val and test are: \\n{}, \\n{}, \\n{}\".format(train_enc_x.shape[0], val_enc_x.shape[0], \\\n",
    "                                                                        test_encoding.shape[0]))\n",
    "# TRAIN STARTS\n",
    "save_model = os.path.join(data_dir, 'temp_saved_model_FCNNets_raw.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    now = datetime.now()\n",
    "    sess.run(init)\n",
    "    total_batch = int(train_enc_x.shape[0]/batch_size)\n",
    "    # Training cycle\n",
    "    for epoch in range(n_epochs):\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_idx = np.random.choice(train_enc_x.shape[0], batch_size)\n",
    "            batch_xs = train_enc_x[batch_idx]\n",
    "            batch_ys = train_enc_y[batch_idx]\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cross_entropy], feed_dict={X: batch_xs, y_: batch_ys})\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            val_probs = sess.run(pred_probs, feed_dict={X: val_enc_x})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                  \"cost=\", \"{:.9f}\".format(c), \n",
    "                  \"Val auc=\", \"{:.6f}\".format(auc(val_enc_y[:, 1], val_probs[:, 1])), \n",
    "                  \"Time elapsed=\", \"{}\".format(datetime.now() - now))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    save_path = saver.save(sess, save_model)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC with Cross Softmax Entropy= 92%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .\\temp_saved_model_FCNNets_raw.ckpt\n",
      "Test auc score: 0.944725673503535\n"
     ]
    }
   ],
   "source": [
    "save_model = os.path.join(data_dir, 'temp_saved_model_FCNNets_raw.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    now = datetime.now()\n",
    "    \n",
    "    saver.restore(sess, save_model)\n",
    "    \n",
    "    test_probs = sess.run(pred_probs, feed_dict={X: test_x})\n",
    "    \n",
    "    print(\"Test auc score: {}\".format(auc(test_enc_y[:, 1], test_probs[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge loss entropy with 40 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim for test_encoding and train_encoding are: \n",
      " (71202, 15) \n",
      " (213605, 15)\n"
     ]
    }
   ],
   "source": [
    " print(\"Dim for test_encoding and train_encoding are: \\n\", test_encoding.shape, '\\n', train_encoding.shape)\n",
    "#n_input = test_encoding.shape[1]\n",
    "n_input1 = test_encoding.shape[1]\n",
    "\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_input1], name='input_x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None, output_size], name='target_y')\n",
    "\n",
    "weights = {\n",
    "   'W1': tf.Variable(tf.truncated_normal([n_input1, hidden_size])),\n",
    "   'W2': tf.Variable(tf.truncated_normal([hidden_size, output_size])),\n",
    "}\n",
    "biases = {\n",
    "   'b1': tf.Variable(tf.zeros([hidden_size])),\n",
    "   'b2': tf.Variable(tf.zeros([output_size])),\n",
    "}\n",
    "\n",
    "hidden_layer =  tf.nn.relu(tf.add(tf.matmul(X, weights['W1']), biases['b1']))\n",
    "pred_logits = tf.add(tf.matmul(hidden_layer, weights['W2']), biases['b2'])\n",
    "pred_probs = tf.nn.softmax(pred_logits)\n",
    "\n",
    "cross_entropy1 = tf.losses.hinge_loss(labels=y_, logits=pred_logits)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(2e-4).minimize(cross_entropy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of data for train, val and test are: \n",
      "170884, \n",
      "42721, \n",
      "71202\n",
      "Epoch: 0001 cost= 0.388080537 Val auc= 0.872042 Time elapsed= 0:00:02.361014\n",
      "Epoch: 0002 cost= 0.184989512 Val auc= 0.841299 Time elapsed= 0:00:03.174040\n",
      "Epoch: 0003 cost= 0.104796737 Val auc= 0.815421 Time elapsed= 0:00:03.982850\n",
      "Epoch: 0004 cost= 0.032521646 Val auc= 0.809020 Time elapsed= 0:00:04.768262\n",
      "Epoch: 0005 cost= 0.025235254 Val auc= 0.814739 Time elapsed= 0:00:05.613985\n",
      "Epoch: 0006 cost= 0.015862674 Val auc= 0.832547 Time elapsed= 0:00:06.454091\n",
      "Epoch: 0007 cost= 0.017648680 Val auc= 0.851951 Time elapsed= 0:00:07.349193\n",
      "Epoch: 0008 cost= 0.002816314 Val auc= 0.860884 Time elapsed= 0:00:08.216149\n",
      "Epoch: 0009 cost= 0.000000000 Val auc= 0.859376 Time elapsed= 0:00:09.121139\n",
      "Epoch: 0010 cost= 0.001977457 Val auc= 0.859405 Time elapsed= 0:00:10.045152\n",
      "Epoch: 0011 cost= 0.000000000 Val auc= 0.859200 Time elapsed= 0:00:11.018658\n",
      "Epoch: 0012 cost= 0.006664959 Val auc= 0.860927 Time elapsed= 0:00:11.945272\n",
      "Epoch: 0013 cost= 0.001897810 Val auc= 0.864634 Time elapsed= 0:00:12.871031\n",
      "Epoch: 0014 cost= 0.000000000 Val auc= 0.867351 Time elapsed= 0:00:13.762945\n",
      "Epoch: 0015 cost= 0.002503926 Val auc= 0.870975 Time elapsed= 0:00:14.684419\n",
      "Epoch: 0016 cost= 0.000000000 Val auc= 0.875420 Time elapsed= 0:00:15.590140\n",
      "Epoch: 0017 cost= 0.000474453 Val auc= 0.882762 Time elapsed= 0:00:16.480637\n",
      "Epoch: 0018 cost= 0.000000000 Val auc= 0.887031 Time elapsed= 0:00:17.393833\n",
      "Epoch: 0019 cost= 0.000329450 Val auc= 0.895299 Time elapsed= 0:00:18.385428\n",
      "Epoch: 0020 cost= 0.013228273 Val auc= 0.901229 Time elapsed= 0:00:19.331183\n",
      "Epoch: 0021 cost= 0.000000000 Val auc= 0.904032 Time elapsed= 0:00:20.267466\n",
      "Epoch: 0022 cost= 0.001970012 Val auc= 0.908440 Time elapsed= 0:00:21.213387\n",
      "Epoch: 0023 cost= 0.000000000 Val auc= 0.913462 Time elapsed= 0:00:22.129225\n",
      "Epoch: 0024 cost= 0.001185547 Val auc= 0.915888 Time elapsed= 0:00:23.062376\n",
      "Epoch: 0025 cost= 0.000342131 Val auc= 0.917348 Time elapsed= 0:00:23.987905\n",
      "Epoch: 0026 cost= 0.008071978 Val auc= 0.920545 Time elapsed= 0:00:24.921003\n",
      "Epoch: 0027 cost= 0.008119272 Val auc= 0.923910 Time elapsed= 0:00:25.795001\n",
      "Epoch: 0028 cost= 0.008110810 Val auc= 0.924698 Time elapsed= 0:00:26.668003\n",
      "Epoch: 0029 cost= 0.000297115 Val auc= 0.925046 Time elapsed= 0:00:27.571208\n",
      "Epoch: 0030 cost= 0.000000000 Val auc= 0.925724 Time elapsed= 0:00:28.506707\n",
      "Epoch: 0031 cost= 0.000000000 Val auc= 0.927442 Time elapsed= 0:00:29.369686\n",
      "Epoch: 0032 cost= 0.000000000 Val auc= 0.928468 Time elapsed= 0:00:30.283297\n",
      "Epoch: 0033 cost= 0.000000000 Val auc= 0.927741 Time elapsed= 0:00:31.161503\n",
      "Epoch: 0034 cost= 0.011918913 Val auc= 0.929275 Time elapsed= 0:00:32.096195\n",
      "Epoch: 0035 cost= 0.000000000 Val auc= 0.929399 Time elapsed= 0:00:33.020405\n",
      "Epoch: 0036 cost= 0.000243275 Val auc= 0.929804 Time elapsed= 0:00:33.967215\n",
      "Epoch: 0037 cost= 0.000000000 Val auc= 0.930250 Time elapsed= 0:00:34.890231\n",
      "Epoch: 0038 cost= 0.000000000 Val auc= 0.930958 Time elapsed= 0:00:35.838056\n",
      "Epoch: 0039 cost= 0.008107261 Val auc= 0.931309 Time elapsed= 0:00:36.753950\n",
      "Epoch: 0040 cost= 0.000270749 Val auc= 0.930919 Time elapsed= 0:00:37.658389\n",
      "Optimization Finished!\n",
      "Model saved in file: .\\temp_saved_model_FCLayers.ckpt\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 256\n",
    "\n",
    "# PREPARE DATA\n",
    "VAL_PERC = 0.2\n",
    "all_y_bin = np.zeros((df.shape[0], 2))\n",
    "all_y_bin[range(df.shape[0]), df['Class'].values] = 1\n",
    "\n",
    "train_enc_x = train_encoding[:int(train_encoding.shape[0] * (1-VAL_PERC))]\n",
    "train_enc_y = all_y_bin[:int(train_encoding.shape[0] * (1-VAL_PERC))]\n",
    "\n",
    "val_enc_x = train_encoding[int(train_encoding.shape[0] * (1-VAL_PERC)):]\n",
    "val_enc_y = all_y_bin[int(train_encoding.shape[0] * (1-VAL_PERC)):train_encoding.shape[0]]\n",
    "\n",
    "test_enc_y = all_y_bin[train_encoding.shape[0]:]\n",
    "print(\"Num of data for train, val and test are: \\n{}, \\n{}, \\n{}\".format(train_enc_x.shape[0], val_enc_x.shape[0], \\\n",
    "                                                                       test_encoding.shape[0]))\n",
    "\n",
    "# TRAIN STARTS\n",
    "save_model = os.path.join(data_dir, 'temp_saved_model_FCLayers.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "   now = datetime.now()\n",
    "   sess.run(init)\n",
    "   total_batch = int(train_enc_x.shape[0]/batch_size)\n",
    "   # Training cycle\n",
    "   for epoch in range(n_epochs):\n",
    "       # Loop over all batches\n",
    "       for i in range(total_batch):\n",
    "           batch_idx = np.random.choice(train_enc_x.shape[0], batch_size)\n",
    "           batch_xs = train_enc_x[batch_idx]\n",
    "           batch_ys = train_enc_y[batch_idx]\n",
    "\n",
    "           # Run optimization op (backprop) and cost op (to get loss value)\n",
    "           _, c = sess.run([optimizer, cross_entropy1], feed_dict={X: batch_xs, y_: batch_ys})\n",
    "           \n",
    "       # Display logs per epoch step\n",
    "       if epoch % display_step == 0:\n",
    "           val_probs = sess.run(pred_probs, feed_dict={X: val_enc_x})\n",
    "           print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                 \"cost=\", \"{:.9f}\".format(c),\n",
    "                 \"Val auc=\", \"{:.6f}\".format(auc(val_enc_y[:, 1], val_probs[:, 1])),\n",
    "                 \"Time elapsed=\", \"{}\".format(datetime.now() - now))\n",
    "\n",
    "   print(\"Optimization Finished!\")\n",
    "   \n",
    "   save_path = saver.save(sess, save_model)\n",
    "   print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## hinge loss entropy\n",
    "AUC:93\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
