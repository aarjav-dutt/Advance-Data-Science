{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREDIT CARD FRAUD DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from datetime import datetime \n",
    "from sklearn.metrics import roc_auc_score as auc \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"creditcards.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time spanning: 2.0 days\n",
      "0.173 % of all transactions are fraud. \n"
     ]
    }
   ],
   "source": [
    "print(\"Total time spanning: {:.1f} days\".format(df['Time'].max() / (3600 * 24.0)))\n",
    "print(\"{:.3f} % of all transactions are fraud. \".format(np.sum(df['Class']) / df.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      float64\n",
       "V1        float64\n",
       "V2        float64\n",
       "V3        float64\n",
       "V4        float64\n",
       "V5        float64\n",
       "V6        float64\n",
       "V7        float64\n",
       "V8        float64\n",
       "V9        float64\n",
       "V10       float64\n",
       "V11       float64\n",
       "V12       float64\n",
       "V13       float64\n",
       "V14       float64\n",
       "V15       float64\n",
       "V16       float64\n",
       "V17       float64\n",
       "V18       float64\n",
       "V19       float64\n",
       "V20       float64\n",
       "V21       float64\n",
       "V22       float64\n",
       "V23       float64\n",
       "V24       float64\n",
       "V25       float64\n",
       "V26       float64\n",
       "V27       float64\n",
       "V28       float64\n",
       "Amount    float64\n",
       "Class       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time and Amount columns have been scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n",
      "C:\\Users\\Shreya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "df['scaled_amount'] = std_scaler.fit_transform(df['Amount'].reshape(-1,1))\n",
    "df['scaled_time'] = std_scaler.fit_transform(df['Time'].reshape(-1,1))\n",
    "\n",
    "df.drop(['Time','Amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.244964</td>\n",
       "      <td>-1.996583</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.342475</td>\n",
       "      <td>-1.996583</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.160686</td>\n",
       "      <td>-1.996562</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.140534</td>\n",
       "      <td>-1.996562</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.073403</td>\n",
       "      <td>-1.996541</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "0       0.244964    -1.996583 -1.359807 -0.072781  2.536347  1.378155   \n",
       "1      -0.342475    -1.996583  1.191857  0.266151  0.166480  0.448154   \n",
       "2       1.160686    -1.996562 -1.358354 -1.340163  1.773209  0.379780   \n",
       "3       0.140534    -1.996562 -0.966272 -0.185226  1.792993 -0.863291   \n",
       "4      -0.073403    -1.996541 -1.158233  0.877737  1.548718  0.403034   \n",
       "\n",
       "         V5        V6        V7        V8  ...         V20       V21  \\\n",
       "0 -0.338321  0.462388  0.239599  0.098698  ...    0.251412 -0.018307   \n",
       "1  0.060018 -0.082361 -0.078803  0.085102  ...   -0.069083 -0.225775   \n",
       "2 -0.503198  1.800499  0.791461  0.247676  ...    0.524980  0.247998   \n",
       "3 -0.010309  1.247203  0.237609  0.377436  ...   -0.208038 -0.108300   \n",
       "4 -0.407193  0.095921  0.592941 -0.270533  ...    0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  Class  \n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_amount = df['scaled_amount']\n",
    "scaled_time = df['scaled_time']\n",
    "\n",
    "df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "df.insert(0, 'scaled_amount', scaled_amount)\n",
    "df.insert(1, 'scaled_time', scaled_time)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test split on time series, using first 75% as training/val, and last 25% as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_RATIO = 0.25\n",
    "df.sort_values('scaled_time', inplace = True)\n",
    "TRA_INDEX = int((1-TEST_RATIO) * df.shape[0])\n",
    "train_x = df.iloc[:TRA_INDEX, 1:-2].values\n",
    "train_y = df.iloc[:TRA_INDEX, -1].values\n",
    "\n",
    "test_x = df.iloc[TRA_INDEX:, 1:-2].values\n",
    "test_y = df.iloc[TRA_INDEX:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train examples: 213605, total fraud cases: 398, equal to 0.00186 of total cases. \n"
     ]
    }
   ],
   "source": [
    "print(\"Total train examples: {}, total fraud cases: {}, equal to {:.5f} of total cases. \".format(train_x.shape[0], np.sum(train_y), np.sum(train_y)/train_x.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test examples: 71202, total fraud cases: 94, equal to 0.00132 of total cases. \n"
     ]
    }
   ],
   "source": [
    "print(\"Total test examples: {}, total fraud cases: {}, equal to {:.5f} of total cases. \".format(test_x.shape[0], np.sum(test_y), np.sum(test_y)/test_y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_mean = []\n",
    "cols_std = []\n",
    "for c in range(train_x.shape[1]):\n",
    "    cols_mean.append(train_x[:,c].mean())\n",
    "    cols_std.append(train_x[:,c].std())\n",
    "    train_x[:, c] = (train_x[:, c] - cols_mean[-1]) / cols_std[-1]\n",
    "    test_x[:, c] =  (test_x[:, c] - cols_mean[-1]) / cols_std[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Modelling and results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auto-encoder as unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "batch_size = 256\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 15 # 1st layer num features\n",
    "#n_hidden_2 = 15 # 2nd layer num features\n",
    "n_input = train_x.shape[1] # MNIST data input (img shape: 28*28)\n",
    "data_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and val the model with 1 hidden layer - Tanh and RMSProp Optimizer- Gaussian Initialization-random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.203795433 Train auc= 0.955355 Time elapsed= 0:00:02.355264\n",
      "Epoch: 0002 cost= 0.736128449 Train auc= 0.954717 Time elapsed= 0:00:04.398700\n",
      "Epoch: 0003 cost= 0.606346786 Train auc= 0.958361 Time elapsed= 0:00:06.579539\n",
      "Epoch: 0004 cost= 0.534644902 Train auc= 0.959066 Time elapsed= 0:00:08.772334\n",
      "Epoch: 0005 cost= 0.515544415 Train auc= 0.959412 Time elapsed= 0:00:10.850908\n",
      "Epoch: 0006 cost= 0.376697451 Train auc= 0.960006 Time elapsed= 0:00:13.043700\n",
      "Epoch: 0007 cost= 0.476166815 Train auc= 0.960606 Time elapsed= 0:00:15.238531\n",
      "Epoch: 0008 cost= 0.474456370 Train auc= 0.960141 Time elapsed= 0:00:17.355162\n",
      "Epoch: 0009 cost= 0.457053006 Train auc= 0.959373 Time elapsed= 0:00:19.373531\n",
      "Epoch: 0010 cost= 0.264274508 Train auc= 0.957305 Time elapsed= 0:00:21.524251\n",
      "Optimization Finished!\n",
      "Model saved in file: .\\temp_saved_model_1layer.ckpt\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "weights = {\n",
    "   'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "  \n",
    "   'decoder_h1': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "  \n",
    "}\n",
    "biases = {\n",
    "   'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "  \n",
    "   'decoder_b1': tf.Variable(tf.random_normal([n_input])),\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "   # Encoder Hidden layer with tanh activation #1\n",
    "   layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                  biases['encoder_b1']))\n",
    " \n",
    "   return layer_1\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "   # Encoder Hidden layer with sigmoid activation #1\n",
    "   layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                  biases['decoder_b1']))\n",
    " \n",
    "   return layer_1\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define batch mse\n",
    "batch_mse = tf.reduce_mean(tf.pow(y_true - y_pred, 2), 1)\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# TRAIN StARTS\n",
    "save_model = os.path.join(data_dir, 'temp_saved_model_1layer.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "   now = datetime.now()\n",
    "   sess.run(init)\n",
    "   total_batch = int(train_x.shape[0]/batch_size)\n",
    "   # Training cycle\n",
    "   for epoch in range(training_epochs):\n",
    "       # Loop over all batches\n",
    "       for i in range(total_batch):\n",
    "           batch_idx = np.random.choice(train_x.shape[0], batch_size)\n",
    "           batch_xs = train_x[batch_idx]\n",
    "           # Run optimization op (backprop) and cost op (to get loss value)\n",
    "           _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs})\n",
    "           \n",
    "       # Display logs per epoch step\n",
    "       if epoch % display_step == 0:\n",
    "           train_batch_mse = sess.run(batch_mse, feed_dict={X: train_x})\n",
    "           print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                 \"cost=\", \"{:.9f}\".format(c),\n",
    "                 \"Train auc=\", \"{:.6f}\".format(auc(train_y, train_batch_mse)),\n",
    "                 \"Time elapsed=\", \"{}\".format(datetime.now() - now))\n",
    "\n",
    "   print(\"Optimization Finished!\")\n",
    "   \n",
    "   save_path = saver.save(sess, save_model)\n",
    "   print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get auto-encoder embedding (the `encoder_op` tensor) for both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .\\temp_saved_model_1layer.ckpt\n",
      "Dim for test_encoding and train_encoding are: \n",
      " (71202, 15) \n",
      " (213605, 15)\n"
     ]
    }
   ],
   "source": [
    "save_model = os.path.join(data_dir, 'temp_saved_model_1layer.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    now = datetime.now()\n",
    "    saver.restore(sess, save_model)\n",
    "    \n",
    "    test_encoding = sess.run(encoder_op, feed_dict={X: test_x})\n",
    "    train_encoding = sess.run(encoder_op, feed_dict={X: train_x})\n",
    "    \n",
    "    print(\"Dim for test_encoding and train_encoding are: \\n\", test_encoding.shape, '\\n', train_encoding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossEntropy-truncated_normal- ADAMOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#n_input = test_encoding.shape[1]\n",
    "n_input = test_encoding.shape[1]\n",
    "\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_input], name='input_x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None, output_size], name='target_y')\n",
    "\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.truncated_normal([n_input, hidden_size])),\n",
    "    'W2': tf.Variable(tf.truncated_normal([hidden_size, output_size])),\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros([hidden_size])),\n",
    "    'b2': tf.Variable(tf.zeros([output_size])),\n",
    "}\n",
    "\n",
    "hidden_layer =  tf.nn.relu(tf.add(tf.matmul(X, weights['W1']), biases['b1']))\n",
    "pred_logits = tf.add(tf.matmul(hidden_layer, weights['W2']), biases['b2'])\n",
    "pred_probs = tf.nn.softmax(pred_logits)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=pred_logits))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(2e-4).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data set\n",
    "### Now we need to re-split the train into train/val, due to supervised training.\n",
    "### We will therefore use 80% of out previous training data as our new training, and the remaining 20% as new validation.  \n",
    "### AUC 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of data for train, val and test are: \n",
      "170884, \n",
      "42721, \n",
      "71202\n",
      "Epoch: 0001 cost= 1.155563951 Val auc= 0.447061 Time elapsed= 0:00:01.179452\n",
      "Epoch: 0002 cost= 0.389483750 Val auc= 0.564113 Time elapsed= 0:00:01.821158\n",
      "Epoch: 0003 cost= 0.160097718 Val auc= 0.682188 Time elapsed= 0:00:02.451835\n",
      "Epoch: 0004 cost= 0.065608770 Val auc= 0.754667 Time elapsed= 0:00:03.313175\n",
      "Epoch: 0005 cost= 0.061806075 Val auc= 0.781916 Time elapsed= 0:00:04.001958\n",
      "Epoch: 0006 cost= 0.029504806 Val auc= 0.790426 Time elapsed= 0:00:04.637650\n",
      "Epoch: 0007 cost= 0.011212895 Val auc= 0.793379 Time elapsed= 0:00:05.306610\n",
      "Epoch: 0008 cost= 0.027531363 Val auc= 0.795063 Time elapsed= 0:00:06.103549\n",
      "Epoch: 0009 cost= 0.013827392 Val auc= 0.797254 Time elapsed= 0:00:06.969855\n",
      "Epoch: 0010 cost= 0.006277842 Val auc= 0.799983 Time elapsed= 0:00:07.801095\n",
      "Epoch: 0011 cost= 0.039207526 Val auc= 0.801882 Time elapsed= 0:00:08.671378\n",
      "Epoch: 0012 cost= 0.004125549 Val auc= 0.805078 Time elapsed= 0:00:09.427389\n",
      "Epoch: 0013 cost= 0.053728241 Val auc= 0.811497 Time elapsed= 0:00:10.309828\n",
      "Epoch: 0014 cost= 0.002443706 Val auc= 0.818072 Time elapsed= 0:00:11.178082\n",
      "Epoch: 0015 cost= 0.001933776 Val auc= 0.828458 Time elapsed= 0:00:12.017282\n",
      "Epoch: 0016 cost= 0.000513910 Val auc= 0.839211 Time elapsed= 0:00:12.870550\n",
      "Epoch: 0017 cost= 0.001713517 Val auc= 0.849062 Time elapsed= 0:00:13.748885\n",
      "Epoch: 0018 cost= 0.000796973 Val auc= 0.858966 Time elapsed= 0:00:14.626216\n",
      "Epoch: 0019 cost= 0.000594406 Val auc= 0.864102 Time elapsed= 0:00:15.412630\n",
      "Epoch: 0020 cost= 0.000777972 Val auc= 0.868516 Time elapsed= 0:00:16.265624\n",
      "Epoch: 0021 cost= 0.001455923 Val auc= 0.871721 Time elapsed= 0:00:17.129877\n",
      "Epoch: 0022 cost= 0.000978831 Val auc= 0.874778 Time elapsed= 0:00:17.985155\n",
      "Epoch: 0023 cost= 0.000833860 Val auc= 0.876809 Time elapsed= 0:00:18.836417\n",
      "Epoch: 0024 cost= 0.000656808 Val auc= 0.877130 Time elapsed= 0:00:19.775913\n",
      "Epoch: 0025 cost= 0.001250795 Val auc= 0.878392 Time elapsed= 0:00:20.597103\n",
      "Epoch: 0026 cost= 0.000513875 Val auc= 0.879865 Time elapsed= 0:00:21.446461\n",
      "Epoch: 0027 cost= 0.000579962 Val auc= 0.881943 Time elapsed= 0:00:22.287597\n",
      "Epoch: 0028 cost= 0.033284958 Val auc= 0.884902 Time elapsed= 0:00:23.134850\n",
      "Epoch: 0029 cost= 0.000625014 Val auc= 0.886554 Time elapsed= 0:00:23.992133\n",
      "Epoch: 0030 cost= 0.005961650 Val auc= 0.886948 Time elapsed= 0:00:24.859805\n",
      "Epoch: 0031 cost= 0.000597164 Val auc= 0.888118 Time elapsed= 0:00:25.705686\n",
      "Epoch: 0032 cost= 0.000405834 Val auc= 0.889235 Time elapsed= 0:00:26.509828\n",
      "Epoch: 0033 cost= 0.001521510 Val auc= 0.889879 Time elapsed= 0:00:27.371120\n",
      "Epoch: 0034 cost= 0.000424161 Val auc= 0.891334 Time elapsed= 0:00:28.231405\n",
      "Epoch: 0035 cost= 0.000856574 Val auc= 0.892357 Time elapsed= 0:00:29.077705\n",
      "Epoch: 0036 cost= 0.000326662 Val auc= 0.893172 Time elapsed= 0:00:29.964014\n",
      "Epoch: 0037 cost= 0.000293869 Val auc= 0.893675 Time elapsed= 0:00:30.788227\n",
      "Epoch: 0038 cost= 0.000462475 Val auc= 0.894652 Time elapsed= 0:00:31.612398\n",
      "Epoch: 0039 cost= 0.001283466 Val auc= 0.894787 Time elapsed= 0:00:32.463665\n",
      "Epoch: 0040 cost= 0.000466194 Val auc= 0.894858 Time elapsed= 0:00:33.310916\n",
      "Epoch: 0041 cost= 0.001146351 Val auc= 0.894832 Time elapsed= 0:00:34.163251\n",
      "Epoch: 0042 cost= 0.022114590 Val auc= 0.895388 Time elapsed= 0:00:35.026522\n",
      "Epoch: 0043 cost= 0.001215464 Val auc= 0.895845 Time elapsed= 0:00:35.889820\n",
      "Epoch: 0044 cost= 0.000670531 Val auc= 0.896219 Time elapsed= 0:00:36.753072\n",
      "Epoch: 0045 cost= 0.001328956 Val auc= 0.896166 Time elapsed= 0:00:37.598367\n",
      "Epoch: 0046 cost= 0.051071327 Val auc= 0.896555 Time elapsed= 0:00:38.469640\n",
      "Epoch: 0047 cost= 0.001559603 Val auc= 0.896569 Time elapsed= 0:00:39.312029\n",
      "Epoch: 0048 cost= 0.000303888 Val auc= 0.896965 Time elapsed= 0:00:40.175173\n",
      "Epoch: 0049 cost= 0.001198137 Val auc= 0.897331 Time elapsed= 0:00:40.964275\n",
      "Epoch: 0050 cost= 0.000390768 Val auc= 0.897619 Time elapsed= 0:00:41.790470\n",
      "Epoch: 0051 cost= 0.001139411 Val auc= 0.897706 Time elapsed= 0:00:42.619790\n",
      "Epoch: 0052 cost= 0.001328532 Val auc= 0.897691 Time elapsed= 0:00:43.322546\n",
      "Epoch: 0053 cost= 0.001229410 Val auc= 0.897979 Time elapsed= 0:00:43.991324\n",
      "Epoch: 0054 cost= 0.000635087 Val auc= 0.897890 Time elapsed= 0:00:44.768422\n",
      "Epoch: 0055 cost= 0.001642710 Val auc= 0.898154 Time elapsed= 0:00:45.619654\n",
      "Epoch: 0056 cost= 0.000437613 Val auc= 0.898216 Time elapsed= 0:00:46.346589\n",
      "Epoch: 0057 cost= 0.000324195 Val auc= 0.898838 Time elapsed= 0:00:47.194845\n",
      "Epoch: 0058 cost= 0.000390995 Val auc= 0.898999 Time elapsed= 0:00:48.021087\n",
      "Epoch: 0059 cost= 0.000795349 Val auc= 0.899045 Time elapsed= 0:00:48.829234\n",
      "Epoch: 0060 cost= 0.001114525 Val auc= 0.899317 Time elapsed= 0:00:49.664418\n",
      "Epoch: 0061 cost= 0.001341811 Val auc= 0.899285 Time elapsed= 0:00:50.557834\n",
      "Epoch: 0062 cost= 0.000358205 Val auc= 0.899444 Time elapsed= 0:00:51.412065\n",
      "Epoch: 0063 cost= 0.000602537 Val auc= 0.899350 Time elapsed= 0:00:52.260362\n",
      "Epoch: 0064 cost= 0.000421120 Val auc= 0.899506 Time elapsed= 0:00:53.114592\n",
      "Epoch: 0065 cost= 0.000363980 Val auc= 0.899736 Time elapsed= 0:00:53.897675\n",
      "Epoch: 0066 cost= 0.035335764 Val auc= 0.899922 Time elapsed= 0:00:54.615583\n",
      "Epoch: 0067 cost= 0.000835867 Val auc= 0.900091 Time elapsed= 0:00:55.438772\n",
      "Epoch: 0068 cost= 0.000371073 Val auc= 0.900162 Time elapsed= 0:00:56.262966\n",
      "Epoch: 0069 cost= 0.002266695 Val auc= 0.900378 Time elapsed= 0:00:57.044043\n",
      "Epoch: 0070 cost= 0.000472129 Val auc= 0.900376 Time elapsed= 0:00:57.808072\n",
      "Epoch: 0071 cost= 0.000293112 Val auc= 0.900474 Time elapsed= 0:00:58.608201\n",
      "Epoch: 0072 cost= 0.001547165 Val auc= 0.900487 Time elapsed= 0:00:59.432395\n",
      "Epoch: 0073 cost= 0.000422034 Val auc= 0.900484 Time elapsed= 0:01:00.264608\n",
      "Epoch: 0074 cost= 0.000305824 Val auc= 0.900709 Time elapsed= 0:01:01.133958\n",
      "Epoch: 0075 cost= 0.000938196 Val auc= 0.900505 Time elapsed= 0:01:02.022283\n",
      "Epoch: 0076 cost= 0.000527831 Val auc= 0.900612 Time elapsed= 0:01:02.881606\n",
      "Epoch: 0077 cost= 0.001513846 Val auc= 0.900491 Time elapsed= 0:01:03.768018\n",
      "Epoch: 0078 cost= 0.026796080 Val auc= 0.900680 Time elapsed= 0:01:04.642347\n",
      "Epoch: 0079 cost= 0.000241895 Val auc= 0.900594 Time elapsed= 0:01:05.511563\n",
      "Epoch: 0080 cost= 0.001784885 Val auc= 0.900711 Time elapsed= 0:01:06.394917\n",
      "Optimization Finished!\n",
      "Model saved in file: .\\temp_saved_model_FCLayers.ckpt\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 80\n",
    "batch_size = 256\n",
    "\n",
    "# PREPARE DATA\n",
    "VAL_PERC = 0.2\n",
    "all_y_bin = np.zeros((df.shape[0], 2))\n",
    "all_y_bin[range(df.shape[0]), df['Class'].values] = 1\n",
    "\n",
    "train_enc_x = train_encoding[:int(train_encoding.shape[0] * (1-VAL_PERC))]\n",
    "train_enc_y = all_y_bin[:int(train_encoding.shape[0] * (1-VAL_PERC))]\n",
    "\n",
    "val_enc_x = train_encoding[int(train_encoding.shape[0] * (1-VAL_PERC)):]\n",
    "val_enc_y = all_y_bin[int(train_encoding.shape[0] * (1-VAL_PERC)):train_encoding.shape[0]]\n",
    "\n",
    "test_enc_y = all_y_bin[train_encoding.shape[0]:]\n",
    "print(\"Num of data for train, val and test are: \\n{}, \\n{}, \\n{}\".format(train_enc_x.shape[0], val_enc_x.shape[0], \\\n",
    "                                                                        test_encoding.shape[0]))\n",
    "\n",
    "# TRAIN STARTS\n",
    "save_model = os.path.join(data_dir, 'temp_saved_model_FCLayers.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    now = datetime.now()\n",
    "    sess.run(init)\n",
    "    total_batch = int(train_enc_x.shape[0]/batch_size)\n",
    "    # Training cycle\n",
    "    for epoch in range(n_epochs):\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_idx = np.random.choice(train_enc_x.shape[0], batch_size)\n",
    "            batch_xs = train_enc_x[batch_idx]\n",
    "            batch_ys = train_enc_y[batch_idx]\n",
    "\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cross_entropy], feed_dict={X: batch_xs, y_: batch_ys})\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            val_probs = sess.run(pred_probs, feed_dict={X: val_enc_x})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                  \"cost=\", \"{:.9f}\".format(c), \n",
    "                  \"Val auc=\", \"{:.6f}\".format(auc(val_enc_y[:, 1], val_probs[:, 1])), \n",
    "                  \"Time elapsed=\", \"{}\".format(datetime.now() - now))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    save_path = saver.save(sess, save_model)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model on the same test data as before - AUC remains constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .\\temp_saved_model_FCLayers.ckpt\n",
      "\n",
      "Test auc score: 0.9320594444889942\n"
     ]
    }
   ],
   "source": [
    "save_model = os.path.join(data_dir, 'temp_saved_model_FCLayers.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    now = datetime.now()\n",
    "    \n",
    "    saver.restore(sess, save_model)\n",
    "    \n",
    "    test_probs = sess.run(pred_probs, feed_dict={X: test_encoding})\n",
    "    \n",
    "    print(\"\\nTest auc score: {}\".format(auc(test_enc_y[:, 1], test_probs[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test a simple supervisied neural network with 2 layers - without using auto-encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax and Relu with Gradient estimation ADAMOptimizer -changing the layer size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 256\n",
    "\n",
    "#n_input = test_encoding.shape[1]\n",
    "n_input = train_x.shape[1]\n",
    "\n",
    "hidden1_size = 8\n",
    "hidden2_size = 4\n",
    "output_size = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_input], name='input_x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None, output_size], name='target_y')\n",
    "\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.truncated_normal([n_input, hidden1_size])),\n",
    "    'W2': tf.Variable(tf.truncated_normal([hidden1_size, hidden2_size])),\n",
    "    'W3': tf.Variable(tf.truncated_normal([hidden2_size, output_size])),\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros([hidden1_size])),\n",
    "    'b2': tf.Variable(tf.zeros([hidden2_size])),\n",
    "    'b3': tf.Variable(tf.zeros([output_size])),\n",
    "}\n",
    "\n",
    "hidden1_layer =  tf.nn.relu(tf.add(tf.matmul(X, weights['W1']), biases['b1']))\n",
    "hidden2_layer =  tf.nn.relu(tf.add(tf.matmul(hidden1_layer, weights['W2']), biases['b2']))\n",
    "pred_logits = tf.add(tf.matmul(hidden2_layer, weights['W3']), biases['b3'])\n",
    "pred_probs = tf.nn.softmax(pred_logits)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=pred_logits))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data set: Now we need to re-split the train into train/val. \n",
    "### Again, we will use 80% of out previous training data as our new training, and the remaining 20% as new validation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of data for train, val and test are: \n",
      "170884, \n",
      "42721, \n",
      "71202\n",
      "Epoch: 0001 cost= 11.543346405 Val auc= 0.814476 Time elapsed= 0:00:01.916098\n",
      "Epoch: 0002 cost= 7.711556435 Val auc= 0.866564 Time elapsed= 0:00:03.053157\n",
      "Epoch: 0003 cost= 5.879520893 Val auc= 0.903337 Time elapsed= 0:00:04.197203\n",
      "Epoch: 0004 cost= 3.347528219 Val auc= 0.921169 Time elapsed= 0:00:05.329176\n",
      "Epoch: 0005 cost= 1.997199774 Val auc= 0.925576 Time elapsed= 0:00:06.442135\n",
      "Epoch: 0006 cost= 1.826838970 Val auc= 0.920316 Time elapsed= 0:00:07.547078\n",
      "Epoch: 0007 cost= 1.040879726 Val auc= 0.904171 Time elapsed= 0:00:08.527683\n",
      "Epoch: 0008 cost= 0.758096695 Val auc= 0.890673 Time elapsed= 0:00:09.594524\n",
      "Epoch: 0009 cost= 0.522125483 Val auc= 0.864090 Time elapsed= 0:00:10.706482\n",
      "Epoch: 0010 cost= 0.328337401 Val auc= 0.814523 Time elapsed= 0:00:11.799383\n",
      "Epoch: 0011 cost= 0.228455901 Val auc= 0.780235 Time elapsed= 0:00:12.893294\n",
      "Epoch: 0012 cost= 0.146458179 Val auc= 0.771305 Time elapsed= 0:00:13.960131\n",
      "Epoch: 0013 cost= 0.131363615 Val auc= 0.783413 Time elapsed= 0:00:14.974834\n",
      "Epoch: 0014 cost= 0.092965528 Val auc= 0.778260 Time elapsed= 0:00:16.088794\n",
      "Epoch: 0015 cost= 0.118919529 Val auc= 0.787432 Time elapsed= 0:00:17.224820\n",
      "Epoch: 0016 cost= 0.060273532 Val auc= 0.762864 Time elapsed= 0:00:18.349808\n",
      "Epoch: 0017 cost= 0.038742736 Val auc= 0.756550 Time elapsed= 0:00:19.477813\n",
      "Epoch: 0018 cost= 0.046624698 Val auc= 0.761745 Time elapsed= 0:00:20.579738\n",
      "Epoch: 0019 cost= 0.051575661 Val auc= 0.751449 Time elapsed= 0:00:21.692698\n",
      "Epoch: 0020 cost= 0.166499436 Val auc= 0.763372 Time elapsed= 0:00:22.814683\n",
      "Epoch: 0021 cost= 0.013967648 Val auc= 0.785329 Time elapsed= 0:00:23.964742\n",
      "Epoch: 0022 cost= 0.010671353 Val auc= 0.789530 Time elapsed= 0:00:25.102770\n",
      "Epoch: 0023 cost= 0.010461988 Val auc= 0.799581 Time elapsed= 0:00:26.253836\n",
      "Epoch: 0024 cost= 0.009227209 Val auc= 0.802942 Time elapsed= 0:00:27.412914\n",
      "Epoch: 0025 cost= 0.006842834 Val auc= 0.812161 Time elapsed= 0:00:28.557960\n",
      "Epoch: 0026 cost= 0.005732097 Val auc= 0.821291 Time elapsed= 0:00:29.690974\n",
      "Epoch: 0027 cost= 0.004168968 Val auc= 0.822435 Time elapsed= 0:00:30.793906\n",
      "Epoch: 0028 cost= 0.004406897 Val auc= 0.830669 Time elapsed= 0:00:31.758474\n",
      "Epoch: 0029 cost= 0.003908019 Val auc= 0.836075 Time elapsed= 0:00:32.892489\n",
      "Epoch: 0030 cost= 0.004121497 Val auc= 0.839000 Time elapsed= 0:00:33.973368\n",
      "Epoch: 0031 cost= 0.003470694 Val auc= 0.839849 Time elapsed= 0:00:34.981043\n",
      "Epoch: 0032 cost= 0.002386372 Val auc= 0.842344 Time elapsed= 0:00:36.047881\n",
      "Epoch: 0033 cost= 0.002147954 Val auc= 0.843965 Time elapsed= 0:00:37.022475\n",
      "Epoch: 0034 cost= 0.001848829 Val auc= 0.847202 Time elapsed= 0:00:38.021130\n",
      "Epoch: 0035 cost= 0.022528004 Val auc= 0.847149 Time elapsed= 0:00:39.125066\n",
      "Epoch: 0036 cost= 0.032928862 Val auc= 0.850673 Time elapsed= 0:00:40.097652\n",
      "Epoch: 0037 cost= 0.001480016 Val auc= 0.852344 Time elapsed= 0:00:41.251724\n",
      "Epoch: 0038 cost= 0.096471667 Val auc= 0.854344 Time elapsed= 0:00:42.400781\n",
      "Epoch: 0039 cost= 0.001625632 Val auc= 0.855523 Time elapsed= 0:00:43.581921\n",
      "Epoch: 0040 cost= 0.001099403 Val auc= 0.856982 Time elapsed= 0:00:44.755046\n",
      "Epoch: 0041 cost= 0.014950958 Val auc= 0.859357 Time elapsed= 0:00:45.954231\n",
      "Epoch: 0042 cost= 0.001074145 Val auc= 0.859652 Time elapsed= 0:00:47.157432\n",
      "Epoch: 0043 cost= 0.001966544 Val auc= 0.862510 Time elapsed= 0:00:48.357155\n",
      "Epoch: 0044 cost= 0.005488703 Val auc= 0.865592 Time elapsed= 0:00:49.477135\n",
      "Epoch: 0045 cost= 0.000931503 Val auc= 0.867485 Time elapsed= 0:00:50.555001\n",
      "Epoch: 0046 cost= 0.000916105 Val auc= 0.868726 Time elapsed= 0:00:51.570705\n",
      "Epoch: 0047 cost= 0.000759448 Val auc= 0.871462 Time elapsed= 0:00:52.380921\n",
      "Epoch: 0048 cost= 0.000887207 Val auc= 0.871940 Time elapsed= 0:00:53.066679\n",
      "Epoch: 0049 cost= 0.000897929 Val auc= 0.874292 Time elapsed= 0:00:53.864804\n",
      "Epoch: 0050 cost= 0.000828052 Val auc= 0.876445 Time elapsed= 0:00:54.913623\n",
      "Optimization Finished!\n",
      "Model saved in file: .\\temp_saved_model_FCNNets_raw.ckpt\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA\n",
    "VAL_PERC = 0.2\n",
    "all_y_bin = np.zeros((df.shape[0], 2))\n",
    "all_y_bin[range(df.shape[0]), df['Class'].values] = 1\n",
    "\n",
    "train_enc_x = train_x[:int(train_x.shape[0] * (1-VAL_PERC))]\n",
    "train_enc_y = all_y_bin[:int(train_x.shape[0] * (1-VAL_PERC))]\n",
    "\n",
    "val_enc_x = train_x[int(train_encoding.shape[0] *  (1-VAL_PERC)):]\n",
    "val_enc_y = all_y_bin[int(train_encoding.shape[0] * (1-VAL_PERC)):train_x.shape[0]]\n",
    "\n",
    "test_enc_y = all_y_bin[train_x.shape[0]:]\n",
    "\n",
    "print(\"Num of data for train, val and test are: \\n{}, \\n{}, \\n{}\".format(train_enc_x.shape[0], val_enc_x.shape[0], \\\n",
    "                                                                        test_encoding.shape[0]))\n",
    "# TRAIN STARTS\n",
    "save_model = os.path.join(data_dir, 'temp_saved_model_FCNNets_raw.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    now = datetime.now()\n",
    "    sess.run(init)\n",
    "    total_batch = int(train_enc_x.shape[0]/batch_size)\n",
    "    # Training cycle\n",
    "    for epoch in range(n_epochs):\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_idx = np.random.choice(train_enc_x.shape[0], batch_size)\n",
    "            batch_xs = train_enc_x[batch_idx]\n",
    "            batch_ys = train_enc_y[batch_idx]\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cross_entropy], feed_dict={X: batch_xs, y_: batch_ys})\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            val_probs = sess.run(pred_probs, feed_dict={X: val_enc_x})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                  \"cost=\", \"{:.9f}\".format(c), \n",
    "                  \"Val auc=\", \"{:.6f}\".format(auc(val_enc_y[:, 1], val_probs[:, 1])), \n",
    "                  \"Time elapsed=\", \"{}\".format(datetime.now() - now))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    save_path = saver.save(sess, save_model)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC with Cross Softmax Entropy (Cost Function)= 97%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .\\temp_saved_model_FCNNets_raw.ckpt\n",
      "Test auc score: 0.9237228596836218\n"
     ]
    }
   ],
   "source": [
    "save_model = os.path.join(data_dir, 'temp_saved_model_FCNNets_raw.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    now = datetime.now()\n",
    "    \n",
    "    saver.restore(sess, save_model)\n",
    "    \n",
    "    test_probs = sess.run(pred_probs, feed_dict={X: test_x})\n",
    "    \n",
    "    print(\"Test auc score: {}\".format(auc(test_enc_y[:, 1], test_probs[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge loss entropy (Cost Function) with 70 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim for test_encoding and train_encoding are: \n",
      " (71202, 15) \n",
      " (213605, 15)\n"
     ]
    }
   ],
   "source": [
    " print(\"Dim for test_encoding and train_encoding are: \\n\", test_encoding.shape, '\\n', train_encoding.shape)\n",
    "#n_input = test_encoding.shape[1]\n",
    "n_input1 = test_encoding.shape[1]\n",
    "\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_input1], name='input_x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None, output_size], name='target_y')\n",
    "\n",
    "weights = {\n",
    "   'W1': tf.Variable(tf.truncated_normal([n_input1, hidden_size])),\n",
    "   'W2': tf.Variable(tf.truncated_normal([hidden_size, output_size])),\n",
    "}\n",
    "biases = {\n",
    "   'b1': tf.Variable(tf.zeros([hidden_size])),\n",
    "   'b2': tf.Variable(tf.zeros([output_size])),\n",
    "}\n",
    "\n",
    "hidden_layer =  tf.nn.relu(tf.add(tf.matmul(X, weights['W1']), biases['b1']))\n",
    "pred_logits = tf.add(tf.matmul(hidden_layer, weights['W2']), biases['b2'])\n",
    "pred_probs = tf.nn.softmax(pred_logits)\n",
    "\n",
    "cross_entropy1 = tf.losses.hinge_loss(labels=y_, logits=pred_logits)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(2e-4).minimize(cross_entropy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of data for train, val and test are: \n",
      "170884, \n",
      "42721, \n",
      "71202\n",
      "Epoch: 0001 cost= 0.705070674 Val auc= 0.355648 Time elapsed= 0:00:01.649808\n",
      "Epoch: 0002 cost= 0.231795177 Val auc= 0.496578 Time elapsed= 0:00:02.404327\n",
      "Epoch: 0003 cost= 0.089236572 Val auc= 0.642158 Time elapsed= 0:00:03.278571\n",
      "Epoch: 0004 cost= 0.063563339 Val auc= 0.743194 Time elapsed= 0:00:03.968548\n",
      "Epoch: 0005 cost= 0.017952789 Val auc= 0.804824 Time elapsed= 0:00:04.861027\n",
      "Epoch: 0006 cost= 0.014160321 Val auc= 0.819732 Time elapsed= 0:00:05.556774\n",
      "Epoch: 0007 cost= 0.003721347 Val auc= 0.825534 Time elapsed= 0:00:06.437204\n",
      "Epoch: 0008 cost= 0.008375487 Val auc= 0.828642 Time elapsed= 0:00:07.307527\n",
      "Epoch: 0009 cost= 0.002765207 Val auc= 0.829902 Time elapsed= 0:00:08.177747\n",
      "Epoch: 0010 cost= 0.019551896 Val auc= 0.830511 Time elapsed= 0:00:09.103207\n",
      "Epoch: 0011 cost= 0.006081137 Val auc= 0.831850 Time elapsed= 0:00:09.995580\n",
      "Epoch: 0012 cost= 0.000141117 Val auc= 0.833488 Time elapsed= 0:00:10.888957\n",
      "Epoch: 0013 cost= 0.002035155 Val auc= 0.835475 Time elapsed= 0:00:11.780329\n",
      "Epoch: 0014 cost= 0.002047084 Val auc= 0.837146 Time elapsed= 0:00:12.686739\n",
      "Epoch: 0015 cost= 0.000093366 Val auc= 0.839573 Time elapsed= 0:00:13.593148\n",
      "Epoch: 0016 cost= 0.006058261 Val auc= 0.841881 Time elapsed= 0:00:14.496551\n",
      "Epoch: 0017 cost= 0.024019677 Val auc= 0.844170 Time elapsed= 0:00:15.408983\n",
      "Epoch: 0018 cost= 0.011205846 Val auc= 0.848894 Time elapsed= 0:00:16.318398\n",
      "Epoch: 0019 cost= 0.000000000 Val auc= 0.851399 Time elapsed= 0:00:17.201777\n",
      "Epoch: 0020 cost= 0.005428112 Val auc= 0.855065 Time elapsed= 0:00:18.110169\n",
      "Epoch: 0021 cost= 0.000000000 Val auc= 0.859315 Time elapsed= 0:00:19.017578\n",
      "Epoch: 0022 cost= 0.002572656 Val auc= 0.860616 Time elapsed= 0:00:19.941034\n",
      "Epoch: 0023 cost= 0.002929277 Val auc= 0.862132 Time elapsed= 0:00:20.841519\n",
      "Epoch: 0024 cost= 0.000002567 Val auc= 0.863521 Time elapsed= 0:00:21.745836\n",
      "Epoch: 0025 cost= 0.000000000 Val auc= 0.866204 Time elapsed= 0:00:22.664278\n",
      "Epoch: 0026 cost= 0.011897472 Val auc= 0.868840 Time elapsed= 0:00:23.561732\n",
      "Epoch: 0027 cost= 0.009772744 Val auc= 0.869842 Time elapsed= 0:00:24.376835\n",
      "Epoch: 0028 cost= 0.000000000 Val auc= 0.871033 Time elapsed= 0:00:25.140865\n",
      "Epoch: 0029 cost= 0.000000000 Val auc= 0.874658 Time elapsed= 0:00:26.026250\n",
      "Epoch: 0030 cost= 0.000106228 Val auc= 0.877070 Time elapsed= 0:00:26.928623\n",
      "Epoch: 0031 cost= 0.003583350 Val auc= 0.881487 Time elapsed= 0:00:27.712705\n",
      "Epoch: 0032 cost= 0.004136909 Val auc= 0.882962 Time elapsed= 0:00:28.491777\n",
      "Epoch: 0033 cost= 0.003964219 Val auc= 0.884949 Time elapsed= 0:00:29.124459\n",
      "Epoch: 0034 cost= 0.003821788 Val auc= 0.887400 Time elapsed= 0:00:29.954668\n",
      "Epoch: 0035 cost= 0.000000000 Val auc= 0.890505 Time elapsed= 0:00:30.582336\n",
      "Epoch: 0036 cost= 0.004166835 Val auc= 0.893044 Time elapsed= 0:00:31.073645\n",
      "Epoch: 0037 cost= 0.003436516 Val auc= 0.895388 Time elapsed= 0:00:31.556931\n",
      "Epoch: 0038 cost= 0.003305801 Val auc= 0.897868 Time elapsed= 0:00:32.195628\n",
      "Epoch: 0039 cost= 0.003186055 Val auc= 0.899662 Time elapsed= 0:00:32.975704\n",
      "Epoch: 0040 cost= 0.004749061 Val auc= 0.902790 Time elapsed= 0:00:33.825970\n",
      "Epoch: 0041 cost= 0.009772398 Val auc= 0.903626 Time elapsed= 0:00:34.717404\n",
      "Epoch: 0042 cost= 0.000000000 Val auc= 0.905198 Time elapsed= 0:00:35.361048\n",
      "Epoch: 0043 cost= 0.000000000 Val auc= 0.906948 Time elapsed= 0:00:35.984707\n",
      "Epoch: 0044 cost= 0.000000000 Val auc= 0.908801 Time elapsed= 0:00:36.768792\n",
      "Epoch: 0045 cost= 0.013332441 Val auc= 0.909608 Time elapsed= 0:00:37.265112\n",
      "Epoch: 0046 cost= 0.012427140 Val auc= 0.911060 Time elapsed= 0:00:37.766446\n",
      "Epoch: 0047 cost= 0.002186902 Val auc= 0.911856 Time elapsed= 0:00:38.340974\n",
      "Epoch: 0048 cost= 0.021245923 Val auc= 0.913006 Time elapsed= 0:00:39.136090\n",
      "Epoch: 0049 cost= 0.000000000 Val auc= 0.913736 Time elapsed= 0:00:39.901123\n",
      "Epoch: 0050 cost= 0.000000000 Val auc= 0.915131 Time elapsed= 0:00:40.640091\n",
      "Epoch: 0051 cost= 0.020423286 Val auc= 0.916523 Time elapsed= 0:00:41.164485\n",
      "Epoch: 0052 cost= 0.000000000 Val auc= 0.917244 Time elapsed= 0:00:41.874371\n",
      "Epoch: 0053 cost= 0.002732254 Val auc= 0.918453 Time elapsed= 0:00:42.706585\n",
      "Epoch: 0054 cost= 0.000000000 Val auc= 0.919770 Time elapsed= 0:00:43.599963\n",
      "Epoch: 0055 cost= 0.001079979 Val auc= 0.920308 Time elapsed= 0:00:44.496349\n",
      "Epoch: 0056 cost= 0.000000000 Val auc= 0.921099 Time elapsed= 0:00:45.393737\n",
      "Epoch: 0057 cost= 0.016793245 Val auc= 0.920802 Time elapsed= 0:00:46.223940\n",
      "Epoch: 0058 cost= 0.000825741 Val auc= 0.922342 Time elapsed= 0:00:47.086234\n",
      "Epoch: 0059 cost= 0.000198454 Val auc= 0.922852 Time elapsed= 0:00:47.914439\n",
      "Epoch: 0060 cost= 0.000000000 Val auc= 0.923643 Time elapsed= 0:00:48.754672\n",
      "Epoch: 0061 cost= 0.000000000 Val auc= 0.924236 Time elapsed= 0:00:49.521715\n",
      "Epoch: 0062 cost= 0.000000000 Val auc= 0.925661 Time elapsed= 0:00:50.291761\n",
      "Epoch: 0063 cost= 0.009169742 Val auc= 0.926618 Time elapsed= 0:00:51.097907\n",
      "Epoch: 0064 cost= 0.000164059 Val auc= 0.927551 Time elapsed= 0:00:52.000306\n",
      "Epoch: 0065 cost= 0.000000000 Val auc= 0.928777 Time elapsed= 0:00:52.844551\n",
      "Epoch: 0066 cost= 0.000276190 Val auc= 0.929476 Time elapsed= 0:00:53.478237\n",
      "Epoch: 0067 cost= 0.000000000 Val auc= 0.931174 Time elapsed= 0:00:54.296413\n",
      "Epoch: 0068 cost= 0.009417204 Val auc= 0.931869 Time elapsed= 0:00:55.116595\n",
      "Epoch: 0069 cost= 0.000000000 Val auc= 0.933293 Time elapsed= 0:00:55.795401\n",
      "Epoch: 0070 cost= 0.000018323 Val auc= 0.934463 Time elapsed= 0:00:56.638645\n",
      "Optimization Finished!\n",
      "Model saved in file: .\\temp_saved_model_FCLayers.ckpt\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 70\n",
    "batch_size = 256\n",
    "\n",
    "# PREPARE DATA\n",
    "VAL_PERC = 0.2\n",
    "all_y_bin = np.zeros((df.shape[0], 2))\n",
    "all_y_bin[range(df.shape[0]), df['Class'].values] = 1\n",
    "\n",
    "train_enc_x = train_encoding[:int(train_encoding.shape[0] * (1-VAL_PERC))]\n",
    "train_enc_y = all_y_bin[:int(train_encoding.shape[0] * (1-VAL_PERC))]\n",
    "\n",
    "val_enc_x = train_encoding[int(train_encoding.shape[0] * (1-VAL_PERC)):]\n",
    "val_enc_y = all_y_bin[int(train_encoding.shape[0] * (1-VAL_PERC)):train_encoding.shape[0]]\n",
    "\n",
    "test_enc_y = all_y_bin[train_encoding.shape[0]:]\n",
    "print(\"Num of data for train, val and test are: \\n{}, \\n{}, \\n{}\".format(train_enc_x.shape[0], val_enc_x.shape[0], \\\n",
    "                                                                       test_encoding.shape[0]))\n",
    "\n",
    "# TRAIN STARTS\n",
    "save_model = os.path.join(data_dir, 'temp_saved_model_FCLayers.ckpt')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "   now = datetime.now()\n",
    "   sess.run(init)\n",
    "   total_batch = int(train_enc_x.shape[0]/batch_size)\n",
    "   # Training cycle\n",
    "   for epoch in range(n_epochs):\n",
    "       # Loop over all batches\n",
    "       for i in range(total_batch):\n",
    "           batch_idx = np.random.choice(train_enc_x.shape[0], batch_size)\n",
    "           batch_xs = train_enc_x[batch_idx]\n",
    "           batch_ys = train_enc_y[batch_idx]\n",
    "\n",
    "           # Run optimization op (backprop) and cost op (to get loss value)\n",
    "           _, c = sess.run([optimizer, cross_entropy1], feed_dict={X: batch_xs, y_: batch_ys})\n",
    "           \n",
    "       # Display logs per epoch step\n",
    "       if epoch % display_step == 0:\n",
    "           val_probs = sess.run(pred_probs, feed_dict={X: val_enc_x})\n",
    "           print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                 \"cost=\", \"{:.9f}\".format(c),\n",
    "                 \"Val auc=\", \"{:.6f}\".format(auc(val_enc_y[:, 1], val_probs[:, 1])),\n",
    "                 \"Time elapsed=\", \"{}\".format(datetime.now() - now))\n",
    "\n",
    "   print(\"Optimization Finished!\")\n",
    "   \n",
    "   save_path = saver.save(sess, save_model)\n",
    "   print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## hinge loss entropy\n",
    "AUC:88\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test split on time series, using first 75% as training/val, and last 25% as test\n",
    "### Train and val the model with 1 hidden layer - Tanh and RMSProp Optimizer- Gaussian Initialization-random --Scaled(96) Unscaled(93)\n",
    "### AUC with Cross Softmax Entropy- Unscaled data(92%), Scaled data(97%)\n",
    "### AUC with Hinge loss entropy- Unscaled data(87%), Scaled data(88%)\n",
    "## Best accuracy achieved from-  Softmax and Relu with Gradient estimation ADAM Optimizer on scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "The code in the document by Venelin Valkov curiousily-https://github.com/curiousily/Credit-Card-Fraud-Detection-using-Autoencoders-in-Keras/blob/master/LICENSE is licensed under the MIT License https://opensource.org/licenses/MIT\n",
    "\n",
    "The code in the document by cloudacademy- https://github.com/cloudacademy/fraud-detection/blob/master/LICENSE\n",
    "is licensed under the Apache License  http://www.apache.org/licenses/\n",
    "\n",
    "The code in the document by Aymeric Damien- https://github.com/aymericdamien is licensed under the MIT License https://opensource.org/licenses/MIT\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
